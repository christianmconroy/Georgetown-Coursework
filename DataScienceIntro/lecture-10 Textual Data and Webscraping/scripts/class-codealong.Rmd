---
title: "Lecture 11: Textual Data"
author: "Jeff Chen / PPOL670"
output: ioslides_presentation
---

## Working with text

Text and narratives are the main mode that people store and communicate ideas and information. In modern media, the text of news articles, for example, are stored within Hyper Text Markup Language (HTML) and JavaScript (JS) objects. This is in contrast with the typical structured data in, say, a spreadsheet or csv. 


## Web Scraping

The whole point of webscraping is to obtain and structure data that is otherwise locked in a webpage format. 

Set a URL from the This American Life website, starting from episode 300
```{r, warning=FALSE, message=FALSE}
library(rvest)
ep_num <- 300
url <- paste0("https://www.thisamericanlife.org/",ep_num,"/transcript")
tal <- read_html(url)
# Revest goes ahead and does the parsing in your rstudio instance already
```

Extract the title of the episode from the title tag
```{r}
#Get Title
  title <- tal %>%
    html_node("title") %>%
    html_text()
# Node represents the html tags. 
```

Get the content in a specific div tag labeled 'act' that is nested in 'content' 
```{r}
#Get all sections
  sections <- tal %>%
    html_nodes("div.content > div.act") 
# you're getting the div part from the website elements (look at in the browser)
```

Treat each act as an index number, starting from the prologue = 1. We can scrape the act title that is in the h3 tag.

```{r}
# We extract some data within those div sections
# We're just going further into the nests here. 
#Act Number
  act_num <- 1

#Parse
 act_title <- html_nodes(sections[[act_num]], "h3") %>% html_text()
        
#Inner Content
  a <- html_nodes(sections[[act_num]], "div.act-inner > div")
  # Above, we say look in the first section, and within that, look at the subdivisions and save them as asnother list. 
```

Then, for the remainder of the acts, loop through the div content section one act at a time.

```{r}
dialogue <- data.frame()

for(j in 1:length(a)){
    b <- a[j]
    
    #Within each speaking turn
    for(m in 1:length(b)){
      
      #Identify the speaker
      speaker <- html_nodes(b[m], "h4") %>% html_text() 
      if(length(speaker) <=1){
        speaker <- ""
      } 
      
      #The type of speaker (e.g. host, interviewer, subject)
      speaker.type <- html_attr(b[m],"class")

      #Extract what they said and the timing
      line <- html_nodes(b[m], "p") %>% html_text() 
      times <- html_nodes(b[m], "p") %>% html_attr("begin") 
      para <- 1:length(times)
      
      #Log the speaker turn, parapgraph, text, etc.
      if(length(line) >=1 && length(times) >= 1){
        dialogue <- rbind(dialogue, 
                          data.frame(ep_num = ep_num,
                                     title = title,
                                     act_title = act_title, 
                                     act_num = act_num,
                                     speaker = speaker,
                                     speaker_type = speaker.type,
                                     turn = j,
                                     paragraph = para,
                                     times = times,
                                     text = line))
      }
    }
  }

```

# Sentiment Analysis
Sentiment analysis summarizes textual data in terms of attitudes and emotions of the author. 

Load in text processing libraries.
```{r}
# install.packages('tidytext')
library(tidytext) #for easy text manipulation
# install.packages('SnowballC')
library(SnowballC) #for stemming words

# Stemming -> If you have people and peoples, you can chop off the s, and kind of repeat this process
# tidytext comes with sentiment dictionaries that cross reference words and emotions. 
```

## Step 1: Get a sentiment dictionary
Note that you could create your own as well.
```{r}
  dict <- get_sentiments("bing")
```

## Step 2: Parse your text

Mildly clean up dialogue text
```{r}
# Strip out the useless stuff.
dialogue$text <- tolower(as.character(dialogue$text))
dialogue$text <- gsub("[.,]","",dialogue$text)
dialogue$text <- gsub("[[:digit:]]", "", dialogue$text)
```

Tokenize the text, stem the words, then remove punctuation
```{r}
library(dplyr)
toks <- dialogue %>%
  unnest_tokens(word, text)  %>%
  mutate(word = wordStem(word))
toks$word <- gsub("[[:punct:]]", "", toks$word)

# Take all words and parse them out as single words. 
# Word stem cuts off all the gerands, and plurals, and stuff, and just leaves the root word. 
# toks has all the same as dialogue, but has a new word column and more rows to correspond 
# There's not really a way to account for "not good" being negative. 
## You assume independence of all the words here, which is of course a flaw. 
```

As the stemmed words look odd, we'll create a dictionary of canonical words using the `tm` package in order to fill in the missing endings.
```{r}
# install.packages('tm')
library(tm)

#Import text data
corp <- Corpus(VectorSource(dialogue$text))

#Create term matrix 
tdm <- TermDocumentMatrix(corp, control = list(stemming = TRUE))

#Complete the stems in toks
toks$word <- stemCompletion(toks$word, corp)
```

Also, we'll want to remove stopwords and blank tokens
```{r}
data(stop_words)
toks <- toks %>%
  anti_join(stop_words)
toks <- toks[toks$word != "",]

# Tidy text comes with it's own stop_words 
```

## Step 3: Get Average Sentiment
Get the average positive sentiment in the act.
```{r}
toks <- toks %>% inner_join(dict)
mean(toks$sentiment == "positive")

# What is the portion of words that were positive in the prologue. That's all we downloaded. 
# 16% is highly misleading. 
```

For each paragraph/stanza in sequential order, calculate the mean positive sentiment.
```{r}
out <- aggregate(toks$sentiment=="positive", by = list(speaker_turn = toks$turn), FUN = mean)
plot(out, type = "l") 
```

##DIY! 
Spend 10 minutes to modify the above sentiment analysis code in order to apply it to the first 15 TAL episodes. These have already be scraped and can be downloaded below [Github Repo](https://github.com/SigmaMonstR/getThisAmericanLife). Your goal is to calculate the average positive sentiment for each TAL episode-act. Use the `key` variable that is created below.

Skip the stem completion step as it takes a long time to complete.
```{r}
#Download files
  url <- "https://github.com/SigmaMonstR/getThisAmericanLife/blob/master/data/TAL1to641.Rda?raw=true"
  new_file <- tempfile()
  download.file(url, "temp.Rda", mode = 'wb')
  load("temp.Rda")

#Create keys
  episodes$key <- paste0("ep ", episodes$ep_num, " - act ", episodes$act_num)
  #episodes <- episodes[episodes$ep_num <= 15, ]
```

Reprocessing
```{r}
# Strip out the useless stuff.
episodes$text <- tolower(as.character(episodes$text))
episodes$text <- gsub("[.,]","",episodes$text)
episodes$text <- gsub("[[:digit:]]", "", episodes$text)

toks2 <- episodes %>%
  unnest_tokens(word, text)  %>%
  mutate(word = wordStem(word))
toks2$word <- gsub("[[:punct:]]", "", toks2$word)

#Import text data
corp <- Corpus(VectorSource(episodes$text))

#Create term matrix 
tdm <- TermDocumentMatrix(corp, control = list(stemming = TRUE))


data(stop_words)
toks2 <- toks2 %>%
  anti_join(stop_words)
toks2 <- toks2[toks2$word != "",]

toks2 <- toks2 %>% inner_join(dict)
mean(toks2$sentiment == "positive")

out <- aggregate(toks2$sentiment=="positive", by = list(speaker_turn = toks2$turn), FUN = mean)
plot(out, type = "l") 

```


##Topic Modeling

Goal of topic modeling is to bring structure to textual data -- assign topics and clusters.

First step is not a common step, but is meant to speed up the calculation. We'll take the top 50.

```{r}
#Count words
  toks.count <- episodes %>%
    unnest_tokens(word, text) %>%
    count(key, word, sort = TRUE)  %>%
    anti_join(stop_words)

#Calculate TF-IDF
  toks.count <- toks.count %>%
    bind_tf_idf(word, key, n)
  
#Keep top 50 TFIDF by group
  toks.count <- toks.count %>%
      group_by(key) %>%
      top_n(n = 50, wt = tf_idf)
  
#Create Document Term Matrix
  dtm <- toks.count %>% cast_dtm(key, word, n)
```

##K-means
```{r}
a <- kmeans(dtm.mat, 50)

```

##Alternative currently popular method: Latent Dirichlet Allocaiton
```{r}
#LDA
#install.packages('topicmodels')
  library(topicmodels)
  library(ggplot2)

#Run LDA
 tal_lda <- LDA(dtm, k = 30, control = list(seed = 1234))
 tal_top <- tidy(tal_lda, matrix = "beta")

#Get Beta topics
  tal_top_terms <- tal_top %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)

tal_top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) + 
    theme(text = element_text(size=10)) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()
```