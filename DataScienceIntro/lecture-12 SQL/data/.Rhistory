ggplot(data = rmsr, aes(x = V1, y = value, colour = variable)) + geom_point()
nn_fit <- nnet(z ~ ., data=data, maxit = 2000, decay = .01,  size=2)
set.seed(20305)
data <- data.frame(matrix(rnorm(100), nrow = 100, ncol = 11, byrow = TRUE))
colnames(data)[11] <- "z"
reg1 <- lm(z ~ ., data = data)
summary(reg1)
anova(reg1)
nn_fit <- nnet(z ~ ., data=data, maxit = 2000, decay = .01,  size=2)
nn_fit <- nnet(z ~ ., data=data, maxit = 2000, decay = .01,  size=5)
data <-load('mnist_all.RData')
train <- data.frame(train$n, train$x, train$y)
train <- train[train$train.y == 4 | train$train.y == 7,]
test <- data.frame(test$n, test$x, test$y)
test <- test[test$test.y == 4 | test$test.y == 7,]
lvlc <- sort(sapply(train , function(x) sd(x)), decreasing = TRUE)[1:20]
lvlc <- train[,names(lvlc)]
cor(lvlc)
# X346 and x602 have high variance and low correlation.
spec <- as.formula("factor(train.y) ~ X346 + X602")
fit <- glm(formula = spec, data = train, family=binomial)
train$pred <- predict(fit, type = 'response')
r <- roc(train$train.y, train$pred)
plot(r)
auc(train$train.y, train$pred)
nn_fit <- nnet(formula = spec, data=train, maxit = 2000, decay = .01,  size=1)
train$pred_nn <- predict(nn_fit, type = "r")
r <- roc(train$train.y, train$pred_nn)
plot(r)
auc(train$train.y, train$pred_nn)
nn_fit2 <- nnet(formula = spec, data=train, maxit = 2000, decay = .01,  size=2)
train$pred_nn2 <- predict(nn_fit2, type = "r")
auc(train$train.y, train$pred_nn2)
nn_fit5 <- nnet(formula = spec, data=train, maxit = 2000, decay = .01,  size=5)
train$pred_nn5 <- predict(nn_fit5, type = "r")
auc(train$train.y, train$pred_nn5)
nn_fit10 <- nnet(formula = spec, data=train, maxit = 2000, decay = .01,  size=10)
train$pred_nn10 <- predict(nn_fit10, type = "r")
auc(train$train.y, train$pred_nn10)
test$pred_nn2 <- predict(nn_fit2, newdata = test, type = "r")
auc(test$test.y, test$pred_nn2)
test$pred_nn5 <- predict(nn_fit5, newdata = test, type = "r")
auc(test$test.y, test$pred_nn5)
test$pred_nn10 <- predict(nn_fit10, newdata = test, type = "r")
auc(test$test.y, test$pred_nn10)
set.seed(1)
x = rnorm(50, 0 ,2)
y<-rep(1, length(x))
y[abs(x) < 1] = 0
plot(x,rep(0,length(x)),col=y+1)
data <- data.frame(x, y)
fit <- glm(y ~ x, data = data, family=binomial)
data$pred <- predict(fit, type = 'response')
r <- roc(data$y, data$pred)
plot(r)
auc(data$y, data$pred)
data$x2 <- x^2
ggplot(data, aes(x = x, y = x2, color = factor(y))) + geom_point(size = 1, alpha = 1, na.rm = TRUE)
fit <- glm(y ~ x + x2, data = data, family=binomial)
summary(fit)
#
data$pred_2 <- predict(fit, type = 'response')
#
r <- roc(data$y, data$pred_2)
plot(r)
auc(data$y, data$pred_2)
#Create data
n <- 1000
df <- data.frame(y = 1:n)
df$x <- df$y  - 0.01*df$y^2 + rnorm(n, 0, 500)
library(ggplot2)
##Scatter plot
p = ggplot(df,aes(x = x,y = y))  +
xlab("x") +
ylab("y")
p1 = p + geom_point() + ggtitle("scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy") +
theme_bw() + ggtitle("scatter (alpha = 0.1)")
par(mar = c(0, 0, 0, 0))
p2 = p + geom_point(alpha = 0.1, colour="navy") +
theme_bw() + ggtitle("scatter (alpha = 0.1)")
##Scatter plot
p = ggplot(df,aes(x = x,y = y))  +
xlab("x") +
ylab("y")
p1 = p + geom_point() + ggtitle("scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy") +
theme_bw() + ggtitle("scatter (alpha = 0.1)")
##Hexbin
p3 = p +
stat_bin_hex(colour = "white", na.rm = TRUE, alpha = 0.9) +
scale_fill_gradientn(colours=c("lightgrey","navy"),
name = "Frequency", na.value = NA) +
guides(fill=FALSE) +
ggtitle("hexbin")
# stat_bin_hex is statistical binning for hexagons. Uses hexagons instead of points.
#Scatter by Group
p4  = ggplot(df,aes(x = x, y = y,
colour = as.factor(round(x*10,1)*20))) +
ggtitle("scatter by group") +
theme(legend.position="none") +
geom_point(alpha = 0.3)
# We create random factors to use as colors above.
p5  = p + ggtitle("contour")+
geom_density2d() +
theme_bw()
p6 = ggplot(df, aes(x = x, y = y)) +
ggtitle("scatter + regression line")+
geom_point(shape = 1,alpha = 0.6, colour="navy") +
geom_smooth()
#Put all graphs onto one canvas
install.packages('gridExtra')
library(gridExtra)
grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3)
temp <- tempfile()
url <- "https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_pia.zip"
download.file(url, temp, mode="wb")
unz <- unzip(temp, exdir=getwd())
acs <- read.csv(unz[1])
str(acs[,1:20])
var_list <- c("HICOV","RAC1P","MAR","SEX","ESR","CIT","AGEP","PINCP","POVPIP","WKHP","SCHL")
df <- acs[acs$AGEP>=16, var_list]
#Healthcare coverage (target variable):
#Create one binary variable for calculations
df$coverage[df$HICOV == 1] <- 0
df$coverage[df$HICOV == 2] <- 1
#another with characters
df$hicov2[df$HICOV == 1] <- "With Healthcare"
df$hicov2[df$HICOV == 2] <- "Without Healthcare"
#Gender
df$sex2[df$SEX == 1] <- "Male"
df$sex2[df$SEX == 2] <- "Female"
#Race
df$race2[df$RAC1P == 1] <- "White alone"
df$race2[df$RAC1P == 2] <- "Black or African Amer. alone"
df$race2[df$RAC1P == 3] <- "Amer. Indian alone"
df$race2[df$RAC1P == 4] <- "Alaska Native alone"
df$race2[df$RAC1P == 5] <- "Amer. Indian + Alaska Nat. tribes"
df$race2[df$RAC1P == 6] <- "Asian alone"
df$race2[df$RAC1P == 7] <- "Nat. Hawaiian + Other Pac. Isl."
df$race2[df$RAC1P == 8] <- "Some other race alone"
df$race2[df$RAC1P == 9] <- "Two or more"
#Marital Status
df$mar2[df$MAR == 1] <- "Married"
df$mar2[df$MAR == 2] <- "Widowed"
df$mar2[df$MAR == 3] <- "Divorced"
df$mar2[df$MAR == 4] <- "Separated"
df$mar2[df$MAR == 5] <- "Never Married"
#Employment Status
df$esr2[df$ESR %in% c(1, 2, 4, 5)] <- "Employed"
df$esr2[df$ESR == 3] <- "Unemployed"
df$esr2[df$ESR == 6] <- "Not in labor force"
#Citizenship
df$cit2[df$CIT %in% c(1, 2, 3, 4)] <- "Citizen"
df$cit2[df$CIT == 5] <- "Not citizen"
#School
df$schl2[df$SCHL<16 ] <- "Less than HS"
df$schl2[df$SCHL>=16 & df$SCHL<21] <- "HS Degree"
df$schl2[df$SCHL==21] <- "Undergraduate Degree"
df$schl2[df$SCHL>21] <- "Graduate Degree"
##Comparison of education attainment vs. healthcare coverage
tab <- table(df$schl2, df$hicov2)
#Get proportions by row
prop.table(tab, 1)
# If we change it to 2, we'd get column-wise proportions.
#Chi-square
chisq.test(tab)
#Set the variables
master <- data.frame(var = c("esr2", "mar2", "race2", "sex2", "schl2", "cit2"),
descrip = c("Employment", "Marital Status",
"Race", "Sex", "Education", "Citizenship"))
master[,1] <- as.character(master[,1])
master[,2] <- as.character(master[,2])
#Loop through each variable and print result
for(i in 1:nrow(master)){
print(master[i, 2])
tab <- table(df[, master[i, 1]], df$hicov2)
print(prop.table(tab, 1))
print(chisq.test(tab))
}
library(ggplot2)
ggplot(df, aes(x = AGEP)) + geom_histogram()
# If we wanted this to be discrete, we would just add binwidth = 1 in the geo_histogram section
ggplot(df, aes(x = PINCP)) + geom_histogram()
ggplot(df, aes(x =  PINCP)) + geom_histogram(fill = "navy") + scale_x_log10() +
ggtitle("Histogram: log10(Personal Income)") +
theme(plot.title = element_text(size=10))
ggplot(df, aes(x = PINCP)) + geom_density(fill = "navy") + scale_x_log10() +
ggtitle("Kernel Density: log10(Personal Income)") +
theme(plot.title = element_text(size=10))
# This is useful for the redistricting analysis I was trying to do! Kernel density is a rolling count. If there were little dips in the distribution, a kernel density would be able to capture that better than a histogram.
ggplot(df, aes(x = AGEP, colours = esr2, fill = esr2)) +
geom_density(alpha = 0.2) +
ggtitle("Employment Status by Age") +
theme(plot.title = element_text(size=10))
# Kernal density by employment status. Little divots probably would not be captured as well using a histogram.
ggplot(df, aes(x = AGEP, colours = hicov2, fill = hicov2)) +
geom_density(alpha = 0.2) +
ggtitle("Healthcare by Age") +
theme(plot.title = element_text(size=10))
ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point()
ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point(alpha = 0.01) + scale_y_log10()
ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point(alpha = 0.01) + scale_y_log10() +
geom_smooth()
ggplot(df, aes(x = AGEP, y = coverage))  + geom_smooth() +
labs(x = "Age (years)", y = "% w/o Coverage (1.0 = 100%)")
ggplot(df, aes(x = PINCP, y = coverage)) + geom_smooth() +
labs(x = "log(Personal Income)", y = "% w/o Coverage (1.0 = 100%)") +
scale_x_log10()
ggplot(df, aes(x = WKHP, y = coverage)) + geom_smooth() +
labs(x = "Hours Worked Per Week", y = "% w/o Coverage (1.0 = 100%)")
ggplot(df, aes(x = POVPIP, y = coverage))+ geom_smooth() +
labs(x = "Poverty Level (100 = at level)", y = "% w/o Coverage (1.0 = 100%)")
set.seed(123)
x <- round(rnorm(100,50,10))
y1 <- c("a","a",NA,"c","d","e", NA,"f", NA,"g","a", NA, "a","c","z","g")
y2 <- c(1, 10, NA, 5, 6, 1, NA, NA, 9, NA, 15, 3, NA, NA, 3, 9, 2)
y3 <- c("z","z",NA,"c","d","e", NA,"f", NA,"s","z", NA, "a","c","z","g")
y4 <- c(2, 11, NA, 3, NA, 5, 6, 1, NA, NA, 9, NA, 16, 3, NA, NA, 2, 4, 3)
df <- data.frame( x1 = c(2, 11, NA, 3, NA, 5, 6, 1, NA, NA, 9, NA, 16, 3, NA, NA, 2, 4, 3),
x2 = c(20, NA, NA, 30, NA, 15, 6, 1, 10, 11, 9, 2, 16, 3, 400, 500, 2, 4, 3),
x3 = c(228, NA, NA, 39, NA, 2, 6, 1, 2, 5, 3, 2, NA, 3, NA, NA, 5, 2, 34))
unique2 <- function(vec){
#create placeholder
p <- c()
#loop
for(i in vec){
if(!(i %in% p)){
p <- c(p, i)
}
}
#return
return(p)
}
#test it
set.seed(123)
x <- round(rnorm(100,50,10))
a <- unique2(x)
#String
y1 <- c("a","a",NA,"c","d","e", NA,"f", NA,"g","a", NA, "a","c","z","g")
tab <- table(y1)
letter <- names(sort(tab,decreasing=TRUE))[1]
y1[is.na(y1)] <- letter
print(y1)
#Numeric
y2 <- c(1, 10, NA, 5, 6, 1, NA, NA, 9, NA, 15, 3, NA, NA, 3, 9, 2)
mu <- mean(y2, na.rm = T)
y2[is.na(y2)] <- mu
print(y2)
#Function
plug.it <- function(vec){
if(class(vec)=="numeric"){
#numeric!
mu <- mean(vec, na.rm = T)
vec[is.na(vec)] <- mu
} else if(class(vec) == "character" | class(vec) == "factor"){
#string!
tab <- table(vec)
letter <- names(sort(tab,decreasing=TRUE))[1]
vec[is.na(vec)] <- letter
}
return(vec)
}
#Test
y3 <- c("z","z",NA,"c","d","e", NA,"f", NA,"s","z", NA, "a","c","z","g")
y4 <- c(2, 11, NA, 3, NA, 5, 6, 1, NA, NA, 9, NA, 16, 3, NA, NA, 2, 4, 3)
plug.it(y3)
plug.it(y4)
df <- data.frame( x1 = c(2, 11, NA, 3, NA, 5, 6, 1, NA, NA, 9, NA, 16, 3, NA, NA, 2, 4, 3),
x2 = c("a","a", NA,"c","d","e", NA,"f", NA,"g","a", NA, "a","c","z","g", "x", "g", "z"),
x3 = c(20, NA, NA, 30, NA, 15, 6, 1, 10, 11, 9, 2, 16, 3, 400, 500, 2, 4, 3),
x4 = c(228, NA, NA, 39, NA, 2, 6, 1, 2, 5, 3, 2, NA, 3, NA, NA, 5, 2, 34))
#Loop it
for(i in 1:ncol(df)){
df[,i] <- plug.it(df[,i])
}
#view
print(df)
((psm1$coefficients[2] * (nobs(psm1)/nrow(FullDataSetForMerge2))) + (psm2$coefficients[2] * (nobs(psm2)/nrow(FullDataSetForMerge2))) + (psm3$coefficients[2] * (nobs(psm3)/nrow(FullDataSetForMerge2))) + (psm4$coefficients[2] * (nobs(psm4)/nrow(FullDataSetForMerge2))) + (psm5$coefficients[2] * (nobs(psm5)/nrow(FullDataSetForMerge2))))
#Create data
n <- 1000
df <- data.frame(y = 1:n)
df$x <- df$y  - 0.01*df$y^2 + rnorm(n, 0, 500)
library(ggplot2)
##Scatter plot
p = ggplot(df,aes(x = x,y = y))  +
xlab("x") +
ylab("y")
p1 = p + geom_point() + ggtitle("scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy") +
theme_bw() + ggtitle("scatter (alpha = 0.1)")
p2
##Hexbin
p3 = p +
stat_bin_hex(colour = "white", na.rm = TRUE, alpha = 0.9) +
scale_fill_gradientn(colours=c("lightgrey","navy"),
name = "Frequency", na.value = NA) +
guides(fill=FALSE) +
ggtitle("hexbin")
# stat_bin_hex is statistical binning for hexagons. Uses hexagons instead of points.
#Scatter by Group
p4  = ggplot(df,aes(x = x, y = y,
colour = as.factor(round(x*10,1)*20))) +
ggtitle("scatter by group") +
theme(legend.position="none") +
geom_point(alpha = 0.3)
# We create random factors to use as colors above.
p5  = p + ggtitle("contour")+
geom_density2d() +
theme_bw()
p6 = ggplot(df, aes(x = x, y = y)) +
ggtitle("scatter + regression line")+
geom_point(shape = 1,alpha = 0.6, colour="navy") +
geom_smooth()
library(gridExtra)
grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3)
library(ggplot2)
#Load data
dir <- ("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-05/data")
setwd(dir)
out <- read.csv("building_permits.csv")
dir <- ("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-05/data")
setwd(dir)
dir <- ("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-05/data")
setwd(dir)
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-05/data")
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-03/ready")
degrees <- read.csv("https://raw.githubusercontent.com/GeorgetownMcCourt/data-science/master/lecture-11/All%20Degrees%20Data.csv")
install.packages('dygraphs')
library(dygraphs)
#Create fake data
x <- 1:1000
y <- sin(x/200) + cos(x/10)
#Create time series object (Takes a vector and allows you to turn it into time series of a certain increment. I've used before!)
ts_obj <- ts(y, frequency = 12, start = c(2010,1))
#Make Graph
dygraph(ts_obj, main = "Example") %>%
dyRangeSelector()
library(quantmod)
# quantmod is used for getting stock price history.
# Super useful!!!
library(dygraphs)
#Get stock tickers using the quantmod library
getSymbols(c("AMZN", "FB"),
src = "yahoo",
from = "2013-01-01",
to = Sys.Date())
library(quantmod)
library(dygraphs)
#Get stock tickers using the quantmod library
invisible(getSymbols(
c("AMZN", "FB", "INTC", "AAPL", "MSFT", "TSLA", "PYPL", "GE", "BA"),
src = "yahoo",
from = "2013-01-01",
to = Sys.Date()))
#Create fake data set
n <- 1000
set.seed(123)
#Set up data to create segments
rand <- c(rnorm(n/2, 10, 5), rnorm(n/2, 30, 10))
#Time
time <- c(rnorm(n/2, 100, 3), rnorm(n/2, 120, 3))
time <- round(24 * ((time - min(time)) / (max(time) - min(time))),0)
time <- paste0(sprintf("%02d", time),"00")
#Coverage Region
region <- c(rep("East", n/2), rep("West", n/2))
#Probability of conversion
prob <- round(runif(n),4)
#Mode of targeting
modet <- c(rep("Direct Mail", 500), rep("Email Promo", 250), rep("Phone Call", 250))
#Unique ID
customer.id  <- paste0(region, "-", 1:n)
#Segments
groups <- paste0("G-", sprintf("%02d", cut(rand, breaks = 10, labels = FALSE)))
#Data
data <- data.frame(customer_id = customer.id,
segment = groups,
prob = prob,
region = region,
time_to_contact = time,
best_channel = modet,
phone = "(XXX) XXX XXXX")
View(data)
library(DT)
# DT library allows you to create an interactive table.
data <- data[order(-data$prob),]
datatable(data, rownames = FALSE)
library(plotly)
agg <- aggregate(data$customer_id, by = list(region = data$region, segment = data$segment), FUN = length)
wide <- reshape(agg,
idvar = "segment",
timevar = "region",
direction = "wide")
colnames(wide)[2:3] <- c("East", "West")
p <- plot_ly(wide, x = ~ segment, y = ~ East, type = 'bar', name = 'East') %>%
add_trace(y = ~West, name = 'West') %>%
layout(yaxis = list(title = 'Count'), barmode = 'group')
p
library(plotly)
#Set up data:
#Roll up East and West
agg1 <- aggregate(customer_id ~ time_to_contact + region,
data = data,
FUN = length)
agg1$customer_id <- agg1$customer_id/10
#Reshape wide
agg.c <- reshape(agg1,
idvar = "time_to_contact",
timevar = "region",
direction = "wide")
agg.c[is.na(agg.c)] <- 0
colnames(agg.c) <- c("time", "staffing.west", "staffing.east")
#Create plot
p <- plot_ly(agg.c, x = ~time) %>%
add_lines(y = ~staffing.west, name = "East") %>%
add_lines(y = ~staffing.east, name = "West", visible = F) %>%
layout(
xaxis = list(domain = c(0.1, 1)),
yaxis = list(title = "Staffing Levels"),
updatemenus = list(
list(
y = 0.7,
buttons = list(
list(method = "restyle",
args = list("visible", list(TRUE, FALSE)),
label = "East"),
list(method = "restyle",
args = list("visible", list(FALSE, TRUE)),
label = "West")))
)
)
p
load("C:/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-12/data/lec12.Rda")
install.packages(c("sqldf", "VennDiagram"))
setwd("C:/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro")
setwd("C:/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-12/data")
library(sqldf)
library(VennDiagram)
setwd("C:/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-12/data")
load("lec12.Rda")
out <- sqldf('SELECT names
FROM us')
out <- sqldf('SELECT names
FROM us')
head(out)
uk <- read.csv("uk-sanctionsconlist.csv")
us <- read.csv("us_screening_list-consolidated_2017-03-31.csv")
View(us)
us2 <- strsplit(as.character(us$alt_name), ";")
alt_id <- data.frame()
for(i in 1:nrow(us)){
if(length(unlist(us2[[i]]))>0){
temp <- data.frame(person = as.character(us$name)[i],
names = unlist(us2[[i]]),
dates_of_birth =  as.character(us$dates_of_birth)[i],
nationalities =  as.character(us$nationalities)[i])
temp2 <- data.frame(person = as.character(us$name)[i],
names = as.character(us$name)[i],
dates_of_birth =  as.character(us$dates_of_birth)[i],
nationalities =  as.character(us$nationalities)[i])
alt_id <- rbind(alt_id, temp, temp2)
print(i)
}
}
us <- alt_id
us$person <- as.character(us$person)
us$names <- as.character(us$names)
us$names <- trimws((us$names))
View(us)
#Clean up UK File
uk <- uk[,c(1:6,8,11,29)]
for(k in 1:ncol(uk)){
uk[,k] <- trimws(uk[,k])
}
load("lec12.Rda")
#load library
##SQLDF is used to run SQL commands
library(sqldf)
library(VennDiagram)
########
#SELECT#
########
#Get comfortable with sqldf
#Basic select: 1 field
out <- sqldf('SELECT names
FROM us')
head(out)
#Basic select: 2 fields
out <- sqldf('SELECT names, dates_of_birth as DOB
FROM us')
#Basic select: 3 fields but first 10 records
out <- sqldf('SELECT names, dates_of_birth as dob, nationalities as nat
FROM us
limit 10')
out <- sqldf('SELECT names, dates_of_birth as DOB
FROM us')
View(out)
out <- sqldf('SELECT names, dates_of_birth as dob, nationalities as nat
FROM us
limit 10')
View(out)
out <- sqldf('SELECT *
FROM us
limit 10')
View(out)
out
head(us, 10)
out <- sqldf('SELECT DISTINCT person
FROM us')
out <- sqldf('SELECT DISTINCT LOWER(person)
FROM us')
out2 <- unique(tolower(us$person))
