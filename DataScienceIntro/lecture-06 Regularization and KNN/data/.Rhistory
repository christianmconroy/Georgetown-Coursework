x_centered = x - c(3,3)
covariance_mat <- (1/99) * t(x_centered) %*% x_centered
eig <- eigen(covariance_mat)
eig$vectors
plot(x_centered)
abline(0, eig$vectors[1,1]/eig$vectors[2,1])
data("iris")
head(iris)
iris0 = iris[,-5]
pairs(iris0,col = rep(2:4, each = 50), lwd = 3)
iris.pca = prcomp(iris0,scale=F)
biplot(iris.pca, cex = .6)
names(iris.pca)
iris.pca$sdev
iris.pca$rotation
var(iris0)
iris.pca$center
iris.pca$scale
head(iris.pca$x)
as.matrix(iris0[1,]-iris.pca$center)%*%as.matrix(iris.pca$rotation) - iris.pca$x[1,]
iris.pca$x[42,1:2]
iris.pca$x[16,1:2]
plot(iris.pca$x[,1],iris.pca$x[,2],col = rep(2:4, each = 50), lwd = 3)
iris.pca1 = prcomp(iris0,scale=T)
plot(iris.pca1$x[,1],iris.pca1$x[,2],col = rep(2:4, each = 50), lwd = 3)
#define a couple useful functions
set.seed(123)
imagegray = function(A){image(A,col = gray((0:255)/256))} # plot grayscale image
mytrunc = function(x, top = 1, bot = 0){pmax(pmin(x,top),bot)}
#define a couple useful functions
set.seed(123)
imagegray = function(A){image(A,col = gray((0:255)/256))} # plot grayscale image
mytrunc = function(x, top = 1, bot = 0){pmax(pmin(x,top),bot)}
A = readJPEG("melencolia.JPG")
library(MASS)
require(jpeg)
require(softImpute)
install.package('pls')
library(MASS)
require(jpeg)
require(softImpute)
install.package('pls')
library(MASS)
require(jpeg)
require(softImpute)
install.packages('pls')
library(pls)
library(ISLR)
iris.pca$center
iris.pca$scale
head(iris.pca$x)
as.matrix(iris0[1,]-iris.pca$center)%*%as.matrix(iris.pca$rotation) - iris.pca$x[1,]
#define a couple useful functions
set.seed(123)
imagegray = function(A){image(A,col = gray((0:255)/256))} # plot grayscale image
mytrunc = function(x, top = 1, bot = 0){pmax(pmin(x,top),bot)}
#define a couple useful functions
set.seed(123)
imagegray = function(A){image(A,col = gray((0:255)/256))} # plot grayscale image
mytrunc = function(x, top = 1, bot = 0){pmax(pmin(x,top),bot)}
A = readJPEG("melencolia.JPG")
install.packages('readJPG')
library(readJPG)
install.packages('readJPEG')
library(readJPEG)
library(MASS)
require(jpeg)
require(softImpute)
# install.packages('pls')
library(pls)
library(ISLR)
library(jpeg)
library(MASS)
require(jpeg)
require(softImpute)
# install.packages('pls')
library(pls)
library(ISLR)
install.packages('jpeg')
library(jpeg)
A = readJPEG("melencolia.JPG")
A = readJPEG("melencolia.JPG")
dim(A)
A.gray = .2989*A[,,1] + .5870*A[,,2] + .1140*A[,,3]
A.gray = t(A.gray[seq(dim(A.gray)[1],1,by = -1),])
A.gray[1:5,1:5]
imagegray(A.gray)
A.svd = svd(A.gray)
names(A.svd)
plot(log10(A.svd$d), main = "Log singular values")
grid(col = 2)
k = 30
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(A.approx+1/2)
imagegray(mytrunc(A.approx))
imagegray(A.gray-A.approx)
A = readJPEG("melencoliaDetail.JPG")
A.gray = .2989*A[,,1] + .5870*A[,,2] + .1140*A[,,3]
A.gray = t(A.gray[seq(dim(A.gray)[1],1,by = -1),])
imagegray(A.gray)
A.svd = svd(A.gray)
k = 2
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(mytrunc(A.approx))
k = 50
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(mytrunc(A.approx))
A = readJPEG("melencolia.JPG")
dim(A)
A.gray = .2989*A[,,1] + .5870*A[,,2] + .1140*A[,,3]
A.gray = t(A.gray[seq(dim(A.gray)[1],1,by = -1),])
A.gray[1:5,1:5]
imagegray(A.gray)
A.svd = svd(A.gray)
names(A.svd)
plot(log10(A.svd$d), main = "Log singular values")
grid(col = 2)
k = 30
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(A.approx+1/2)
imagegray(mytrunc(A.approx))
imagegray(A.gray-A.approx)
k = 100
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(mytrunc(A.approx))
A = readJPEG("melencoliaDetail.JPG")
A.gray = .2989*A[,,1] + .5870*A[,,2] + .1140*A[,,3]
A.gray = t(A.gray[seq(dim(A.gray)[1],1,by = -1),])
imagegray(A.gray)
A.svd = svd(A.gray)
k = 2
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(mytrunc(A.approx))
k = 50
A.approx = A.svd$u[,1:k]%*%diag(A.svd$d[1:k])%*%t(A.svd$v[,1:k])
imagegray(mytrunc(A.approx))
A.inc = A.gray
pixels = dim(A.inc)[1]*dim(A.inc)[2]
deleteratio = .5
incomplete = sample(pixels, deleteratio*pixels, replace = F)
A.inc[incomplete] <- NA
imagegray(A.inc)
A.completion = softImpute(A.inc, rank.max = 100)
install.packages('softImpute')
library(softImpute)
A.completion = softImpute(A.inc, rank.max = 100)
names(A.completion)
A.comp = A.completion$u%*%diag(A.completion$d)%*%t(A.completion$v)
imagegray(mytrunc(A.comp))
A.completion = softImpute(A.inc, rank.max = 100)
names(A.completion)
A.comp = A.completion$u%*%diag(A.completion$d)%*%t(A.completion$v)
imagegray(mytrunc(A.comp))
A.inc.1 = A.gray
A.inc.1[450:500,50:100] <- NA
A.completion = softImpute(A.inc.1, rank.max = 200)
A.comp.1 = A.completion$u%*%diag(A.completion$d)%*%t(A.completion$v)
imagegray(mytrunc(A.comp.1))
head(Hitters)
hitters_df <- Hitters[,c(1:9,19)]
head(hitters_df)
cor(hitters_df[,1:9])
pcr.fit=pcr(Salary ~ ., data=Hitters ,scale=TRUE, validation ="CV")
summary(pcr.fit)
names(pcr.fit)
# performed cross-validation to help select the number of princ comps we should keep
plot(MSEP(pcr.fit))
# other helpful visualizations
plot(pcr.fit,ncomp=3)
plot(pcr.fit, "scores") # plot the data points in the first two PCs
plot(pcr.fit, "loadings")
require(knitr)
#install.packages('glmnet')
require(glmnet)
require(ISLR)
require(MASS)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
load('diabetes.RData')
df <- as.data.frame(cbind(diabetes$y, diabetes$x2))
X.model <- model.matrix(V1 ~ .-1, data = df)
y <- df$V1
# remove the NAs:
y <- na.omit(y)
# Fit ridge
set.seed(1)
fit.ridge <- cv.glmnet(X.model, y, alpha = 0)
X.model <- model.matrix(V1 ~ .-1, data = df)
y <- df$V1
# remove the NAs:
y <- na.omit(y)
# Fit ridge
set.seed(1)
fit.ridge <- cv.glmnet(X.model, y, alpha = 0)
X.model <- model.matrix(V1 ~ .-1, data = df)
y <- df$V1
# remove the NAs:
y <- na.omit(y)
# Fit ridge
set.seed(1)
fit.ridge <- cv.glmnet(X.model, y, alpha = 0)
plot(fit.ridge)
fit.ridge$lambda.1se
sqrt(fit.ridge$cvm[fit.ridge$lambda == fit.ridge$lambda.1se])
# Load The Data
data("College")
# Split into train and test
set.seed(12345)
train <- sample(nrow(College),(nrow(College) * .70), replace = FALSE)
College_train <- College[train,]
College_test <- College[-train,]
# Fit the model on the training data
fit <- lm(Apps ~ ., data = College_train)
# Make predictions on the test dataset here
apps.predicted = predict(fit, newdata = College_test)
# Calculate the root mean squared error
sqrt(mean((apps.predicted - College_test$Apps)^2))
Train.model <- model.matrix(Apps ~ ., data = College_train)
Test.model <- model.matrix(Apps ~ ., data = College_test)
# Fit ridge
fit.ridge <- cv.glmnet(Train.model, College_train$Apps, alpha = 0)
# Get best lambda from fit ridge
fit.ridge.lambda <- fit.ridge$lambda.min
#Predict on Test
apps.predicted <- predict(fit.ridge, s = fit.ridge.lambda, newx = Test.model)
# Calc RMSE
sqrt(mean((apps.predicted - College_test$Apps)^2))
# Fit lasso
fit.lasso <- cv.glmnet(Train.model, College_train$Apps, alpha = 1)
# Get best lambda from fit ridge
fit.lasso.lambda <- fit.lasso$lambda.min
#Predict on Test
apps.predicted <- predict(fit.lasso, s = fit.lasso.lambda, newx = Test.model)
# s = Value(s) of the penalty parameter lambda at which predictions are required.
# Calc RMSE
sqrt(mean((apps.predicted - College_test$Apps)^2))
data("Boston")
# Split into train and test
set.seed(12345)
train <- sample(nrow(Boston),(nrow(Boston) * .70), replace = FALSE)
Boston_train <- Boston[train,]
Boston_test <- Boston[-train,]
Train.model <- model.matrix(medv ~ ., data = Boston_train)
Test.model <- model.matrix(medv ~ ., data = Boston_test)
lasso.mod <- glmnet(Train.model, Boston_train$medv, alpha=1)
plot(lasso.mod, xvar = "lambda")
grid(col = 2)
# Evaluate last five to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s37 > 0,])
# Fit lasso
fit.lasso <- cv.glmnet(Train.model, Boston_train$medv, alpha = 1)
# Get best lambda from fit ridge
fit.lasso.lambda <- fit.lasso$lambda.min
#Predict on Test
medv.predicted <- predict(fit.lasso, s = fit.lasso.lambda, newx = Test.model)
# Report 1se
fit.lasso$lambda.1se
# Calc RMSE
sqrt(mean((medv.predicted - Boston_test$medv)^2))
# Scale all the predictors
Boston_train[, -c(14)] <- scale(Boston_train[, -c(14)])
Boston_test[, -c(14)] <- scale(Boston_test[, -c(14)])
Train.model <- model.matrix(medv ~ ., data = Boston_train)
Test.model <- model.matrix(medv ~ ., data = Boston_test)
lasso.mod <- glmnet(Train.model, Boston_train$medv, alpha=1)
plot(lasso.mod, xvar = "lambda")
grid(col = 2)
# Evaluate last five to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s37 > 0,])
# Fit lasso
fit.lasso <- cv.glmnet(Train.model, Boston_train$medv, alpha = 1)
# Get best lambda from fit ridge
fit.lasso.lambda <- fit.lasso$lambda.min
#Predict on Test
medv.predicted <- predict(fit.lasso, s = fit.lasso.lambda, newx = Test.model)
# Report 1se
fit.lasso$lambda.1se
# Calc RMSE
sqrt(mean((medv.predicted - Boston_test$medv)^2))
mnist <-load('mnist_all.RData')
# Remove NAs
mnist <- na.omit(mnist)
# Train
mnist_train <- data.frame(train$n, train$x, train$y)
mnist_train <- mnist_train[mnist_train$train.y == 1 | mnist_train$train.y == 8,]
mnist_train$train.y <- as.factor(ifelse((mnist_train$train.y == 8), 1, 0))
mnist_train <- mnist_train[ - as.numeric(which(apply(mnist_train, 2, var) == 0))]
# Test
mnist_test <- data.frame(test$n, test$x, test$y)
mnist_test <- mnist_test[mnist_test$test.y == 1 | mnist_test$test.y == 8,]
mnist_test$test.y <- as.factor(ifelse((mnist_test$test.y == 8), 1, 0))
mnist_test <- mnist_test[ - as.numeric(which(apply(mnist_test, 2, var) == 0))]
Train.model <- model.matrix(train.y ~ ., data = mnist_train)[,-621]
Test.model <- model.matrix(test.y ~ ., data = mnist_test)[,-534]
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial", lambda = NULL)
plot(lasso.mod, xvar="lambda")
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
Opt[Opt$s7 > 0, c("s7")]
# 11 left in s7, so eliminated lowest coefficient to get to 10.
Train.model <- model.matrix(train.y ~ X236 + X263 + X292 + X320 + X328 + X348 + X355 + X376 + X404 + X410, data = mnist_train)[,-621]
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial", lambda = NULL)
plot(lasso.mod, xvar="lambda")
rss <- function(y) {
sum((y - mean(y))^2)
}
bestsplit = function(x,y){
indices = 1:length(x)
rss0 = rss(y)
# Keep track of how the RSS decreases for every place that I put the piecewise star
rss_changes = double(length(x) - 1)
for (j in 1:length(x)-1){
indexj = indices < j+1
# Base model minus RSS of the left half plus the RSS of the right half
# rss0 is what I start with
rss_changes[j] = rss0 - (rss(y[indexj]) + rss (y[!indexj]))
}
j_best = which.max(rss_changes)
# This will give me where the best is
# Where is best place to make the split in x. given that split, what is my y hat for the lower half and what is my y hat for the upper half. And what's the change in RSS by doing that split.
return(list(xsplit = (x[j_best+1] + x[j_best])/2,
ylower = mean(y[1 : j_best+1]),
yupper = mean(y[j_best:length(y)]),
deltarss = rss_changes[j_best])
)
}
x = seq(0,10,by = .01)
y0 = cos(x/4 + x^2/5)*x^2/20 + x
y = y0 + rnorm(length(x))
mydf = data.frame(x=x,y=y)
rss0 = 1000*var(y)
plot(x,y)
split0 <- bestsplit(x,y)
split0
index <- x < split0$xsplit
x1 <- x[index]
y1 <- y[index]
split1 = bestsplit(x1,y1)
split1
index <- x > split1$xsplit
x2 = x[index]
y2 <- y[index]
split2 = bestsplit(x2,y2)
split2
mytree = tree(y ~x, data = mydf)
install.packages('tree')
require("tree")
require("gbm")
require("randomForest")
require("ISLR")
mytree = tree(y ~x, data = mydf)
names(mytree)
plot(mytree)
text(mytree,pretty = 2)
plot(x,mytree$y, type= 'p', lwd = 2,pch = 46)
# Remember that we have to specify type = vector with decision trees!
y.pred=predict(mytree,mydf,type= "vector")
points(x,y.pred, col = "red",pch = 46)
load("abalone.RData")
load("abalone.RData")
head(abalone)
lm1 <- lm(Rings ~., data=abalone)
summary(lm1)
abalone_tree = tree(Rings~., data=abalone)
plot(abalone_tree)
text(abalone_tree, cex=.65)
# I think it's up to us in terms of what best should be
pruned_tree <- prune.tree(abalone_tree, best=4)
plot(pruned_tree)
text(pruned_tree, cex=.65)
plot(x,mytree$y, type= 'p', lwd = 2,pch = 46)
y.pred=predict(mytree,mydf,type= "vector")
points(x,y.pred, col = "red",pch = 46)
# Split the data in half
set.seed(12345)
datasplit <- sample(nrow(abalone),(nrow(abalone) * .50), replace = FALSE)
abalone_1 <- abalone[datasplit,]
abalone_2 <- abalone[-datasplit,]
abalone_tree1 = tree(Rings~., data=abalone_1)
plot(abalone_tree1)
text(abalone_tree1, cex=.65)
abalone_tree2 = tree(Rings~., data=abalone_2)
plot(abalone_tree2)
text(abalone_tree2, cex=.65)
# The first few features selected as the most important features remain the same.
# Locations of split are different due to the random sampling - Not a big difference early in the tree
# As you get down the tree, some of the features end up being pretty different. (Viscera in one for example)
# Pruning would be a solution here so that they end up less overfit.
load("airfoil.RData")
load("airfoil.RData")
head(airfoil)
lm1 <- lm(Pressure ~., data=airfoil)
summary(lm1)
# Plot
plot(airfoil$Displacement, airfoil$Pressure)
plot(airfoil$Frequency, airfoil$Pressure)
plot(airfoil$AngleOfAttack, airfoil$Pressure)
plot(airfoil$ChordLength, airfoil$Pressure)
plot(airfoil$FreeStreamVelocity, airfoil$Pressure)
cor(airfoil)
# Split into train and test
set.seed(12345)
train <- sample(nrow(airfoil),(nrow(airfoil) * .70), replace = FALSE)
airfoil_train <- airfoil[train,]
airfoil_test <- airfoil[-train,]
lm1 <- lm(Pressure ~., data=airfoil_train)
summary(lm1)
# train GBM model
boost.airfoil  <- gbm(Pressure ~., data= airfoil_train, n.trees=5000, interaction.depth=4)
summary(boost.airfoil)
# Shrinkage rate - 0.1 (How much do you emphasize the wrongs)
# If you can converge to a lower total error, it'll be worth maybe going to a smaller shrinkage
plot(boost.airfoil)
yhat.boost <- predict(boost.airfoil ,newdata = airfoil_test,n.trees=5000)
Test_RSq <- (cor(yhat.boost, airfoil_test$Pressure))^2
Test_RSq
require(knitr)
require(leaps)
require(pROC)
require(readxl)
library(Rtools)
#install.packages('randomForest')
require(randomForest)
#install.packages('gbm')
require(gbm)
require(MASS)
require(ggplot2)
#install.packages('tree')
require(tree)
require(caret)
require(ISLR)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
require(knitr)
require(leaps)
require(pROC)
require(readxl)
install.packages('Rtools')
library(Rtools)
#install.packages('randomForest')
require(randomForest)
#install.packages('gbm')
require(gbm)
require(MASS)
require(ggplot2)
#install.packages('tree')
require(tree)
require(caret)
require(ISLR)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
require(knitr)
require(leaps)
require(pROC)
require(readxl)
#install.packages('randomForest')
require(randomForest)
#install.packages('gbm')
require(gbm)
require(MASS)
require(ggplot2)
#install.packages('tree')
require(tree)
require(caret)
require(ISLR)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
# Load in Data
concrete <- read_excel("Concrete_Data.xls")
# Clean up names
colnames(concrete) <- c("cementkg", "blustfur", "flyash", "water", "superplas", "courseagg", "fineagg", "age", "CCS")
# Load in Data
concrete <- read_excel("Concrete_Data.xls")
# Clean up names
colnames(concrete) <- c("cementkg", "blustfur", "flyash", "water", "superplas", "courseagg", "fineagg", "age", "CCS")
# Load in Data
concrete <- read_excel("Concrete_Data.xls")
# Clean up names
colnames(concrete) <- c("cementkg", "blustfur", "flyash", "water", "superplas", "courseagg", "fineagg", "age", "CCS")
getwd()
require(knitr)
require(leaps)
require(pROC)
require(readxl)
#install.packages('randomForest')
require(randomForest)
#install.packages('gbm')
require(gbm)
require(MASS)
require(ggplot2)
#install.packages('tree')
require(tree)
require(caret)
require(ISLR)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
# Load in Data
concrete <- read_excel("Concrete_Data.xls")
# Clean up names
colnames(concrete) <- c("cementkg", "blustfur", "flyash", "water", "superplas", "courseagg", "fineagg", "age", "CCS")
getwd()
