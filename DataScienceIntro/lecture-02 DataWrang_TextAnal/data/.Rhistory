mnist_test <- data.frame(test$n, test$x, test$y)
mnist_test <- mnist_test[mnist_test$test.y == 1 | mnist_test$test.y == 8,]
mnist_test$test.y <- as.factor(ifelse((mnist_test$test.y == 8), 1, 0))
mnist_test <- mnist_test[ - as.numeric(which(apply(mnist_test, 2, var) == 0))]
Train.model <- model.matrix(train.y ~ ., data = mnist_train)
Test.model <- model.matrix(test.y ~ ., data = mnist_test)
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial")
plot(lasso.mod, xvar="lambda")
mnist <-load('mnist_all.RData')
# Train
mnist_train <- data.frame(train$n, train$x, train$y)
mnist_train <- mnist_train[mnist_train$train.y == 1 | mnist_train$train.y == 8,]
mnist_train$train.y <- as.factor(ifelse((mnist_train$train.y == 8), 1, 0))
mnist_train <- mnist_train[ - as.numeric(which(apply(mnist_train, 2, var) == 0))]
# Test
mnist_test <- data.frame(test$n, test$x, test$y)
mnist_test <- mnist_test[mnist_test$test.y == 1 | mnist_test$test.y == 8,]
mnist_test$test.y <- as.factor(ifelse((mnist_test$test.y == 8), 1, 0))
mnist_test <- mnist_test[ - as.numeric(which(apply(mnist_test, 2, var) == 0))]
Train.model <- model.matrix(train.y ~ ., data = mnist_train)
Test.model <- model.matrix(test.y ~ ., data = mnist_test)
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial")
plot(lasso.mod, xvar="lambda")
Train.model <- model.matrix(train.y ~ ., data = mnist_train)
Test.model <- model.matrix(test.y ~ ., data = mnist_test)
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial")
plot(lasso.mod, xvar="lambda")
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s37 > 0,])
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
rownames(Opt[Opt$s6 > 0,])
View(opt)
View(Opt)
View(Opt)
mnist <-load('mnist_all.RData')
# Remove NAs
mnist <- na.omit(mnist)
# Train
mnist_train <- data.frame(train$n, train$x, train$y)
mnist_train <- mnist_train[mnist_train$train.y == 1 | mnist_train$train.y == 8,]
mnist_train$train.y <- as.factor(ifelse((mnist_train$train.y == 8), 1, 0))
mnist_train <- mnist_train[ - as.numeric(which(apply(mnist_train, 2, var) == 0))]
names(mnist_train)
# Test
mnist_test <- data.frame(test$n, test$x, test$y)
mnist_test <- mnist_test[mnist_test$test.y == 1 | mnist_test$test.y == 8,]
mnist_test$test.y <- as.factor(ifelse((mnist_test$test.y == 8), 1, 0))
mnist_test <- mnist_test[ - as.numeric(which(apply(mnist_test, 2, var) == 0))]
mnist <-load('mnist_all.RData')
# Remove NAs
mnist <- na.omit(mnist)
# Train
mnist_train <- data.frame(train$n, train$x, train$y)
mnist_train <- mnist_train[mnist_train$train.y == 1 | mnist_train$train.y == 8,]
mnist_train$train.y <- as.factor(ifelse((mnist_train$train.y == 8), 1, 0))
mnist_train <- mnist_train[ - as.numeric(which(apply(mnist_train, 2, var) == 0))]
# Test
mnist_test <- data.frame(test$n, test$x, test$y)
mnist_test <- mnist_test[mnist_test$test.y == 1 | mnist_test$test.y == 8,]
mnist_test$test.y <- as.factor(ifelse((mnist_test$test.y == 8), 1, 0))
mnist_test <- mnist_test[ - as.numeric(which(apply(mnist_test, 2, var) == 0))]
names(mnist_test)
Train.model <- model.matrix(train.y ~ ., data = mnist_train)[,-621]
Test.model <- model.matrix(test.y ~ ., data = mnist_test)[,-534]
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial")
plot(lasso.mod, xvar="lambda")
Train.model <- model.matrix(train.y ~ ., data = mnist_train)[,-621]
Test.model <- model.matrix(test.y ~ ., data = mnist_test)[,-534]
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial", lambda = NULL)
plot(lasso.mod, xvar="lambda")
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
rownames(Opt[Opt$s6 > 0,])
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
Opt[Opt$s7 > 0]
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
Opt[Opt$s7 > 0,]
# Evaluate last ten to remain
Opt <- as.data.frame(as.matrix(lasso.mod$beta))
sapply(Opt, function(x) sum(x > 0))
rownames(Opt[Opt$s7 > 0,])
Opt[Opt$s7 > 0, c("s7")]
Train.model <- model.matrix(train.y ~ X236 + X263 + X292 + X320 + X328 + X348 + X355 + X376 + X404 + X410, data = mnist_train)[,-621]
lasso.mod <- glmnet(Train.model, y= mnist_train$train.y, alpha=1, family="binomial", lambda = NULL)
plot(lasso.mod, xvar="lambda")
sum(df$revenue)
set.seed(123)
df <- data.frame(month = seq(1,12,1),
quarter = c(rep(1,3),rep(2,3),rep(3,3),rep(4,3)),
revenue= round(sin(1:12)*100 +200 + rnorm(12,10,50)),
rev_target = round(sin(1:12)*100)+200,
elec_bill = round(sin((1:12)/5)*100 + rnorm(12,100,20)),
social_media = round(runif(12)*300))
df
set.seed(123)
df <- data.frame(month = seq(1,12,1),
quarter = c(rep(1,3),rep(2,3),rep(3,3),rep(4,3)),
revenue= round(sin(1:12)*100 +200 + rnorm(12,10,50)),
rev_target = round(sin(1:12)*100)+200,
elec_bill = round(sin((1:12)/5)*100 + rnorm(12,100,20)),
social_media = round(runif(12)*300))
df
nrow(df)
set.seed(123)
df <- data.frame(month = seq(1,12,1),
quarter = c(rep(1,3),rep(2,3),rep(3,3),rep(4,3)),
revenue= round(sin(1:12)*100 +200 + rnorm(12,10,50)),
rev_target = round(sin(1:12)*100)+200,
elec_bill = round(sin((1:12)/5)*100 + rnorm(12,100,20)),
social_media = round(runif(12)*300))
df
nrow(df$month)
set.seed(123)
df <- data.frame(month = seq(1,12,1),
quarter = c(rep(1,3),rep(2,3),rep(3,3),rep(4,3)),
revenue= round(sin(1:12)*100 +200 + rnorm(12,10,50)),
rev_target = round(sin(1:12)*100)+200,
elec_bill = round(sin((1:12)/5)*100 + rnorm(12,100,20)),
social_media = round(runif(12)*300))
df
nrow(df$month)
set.seed(123)
df <- data.frame(month = seq(1,12,1),
quarter = c(rep(1,3),rep(2,3),rep(3,3),rep(4,3)),
revenue= round(sin(1:12)*100 +200 + rnorm(12,10,50)),
rev_target = round(sin(1:12)*100)+200,
elec_bill = round(sin((1:12)/5)*100 + rnorm(12,100,20)),
social_media = round(runif(12)*300))
df
nrow(df$month)
nrow(df)
sum(df[,3])
sum(df$revenue)
max(df[,6])
max(df$social_media)
sum(df[4:9, 5])
sum(df[df$quarter == 2 | df$quarter == 3, "elec_bill"])
#Explicit
df[7:12, c(3,5)]
df[df$month > 7, c("revenue", "elec_bill")]
#Based on relative position in data
row <- nrow(df)
df[(row-4):row, c(3,5)]
df[min(df$social_media) == df$social_media, 1]
df[max(df$revenue) == df$revenue, 1]
nrow(df[df$revenue > df$rev_target,])
df[df$revenue > df$rev_target, "month"]
offtimes <- df$revenue < df$rev_target
sum(df$rev_target[offtimes] - df$revenue[offtimes])
df[df$month %% 3== 1, 5]
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-02/data")
install.packages(c("devtools", "wordcloud"))
library(devtools)
install.packages('rio')
library(rio)
install_github("SigmaMonstR/digIt", force = TRUE)
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-02/data")
speech10 <- readLines("sotu_2010.txt")
speech10
speech10 <- speech10[speech10 != ""]
ind <- regexpr("Applause", speech10)
ind
sum(attr(ind,"match.length") > 1)
speech16 <- digIt("speech_2016")
speech16 <- digIt("speech_2016")
install_github("SigmaMonstR/digIt", force = TRUE)
speech16 <- digIt("speech_2016")
install_github("SigmaMonstR/digIt", force = TRUE)
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-02/data")
# Install packages
install.packages(c("devtools", "wordcloud"))
library(devtools)
install.packages('rio')
library(rio)
install_github("SigmaMonstR/digIt", force = TRUE)
install.packages(c("devtools", "wordcloud"))
speech16 <- digIt("speech_2016")
speech16 <- readLines("sotu_2016.txt")
speech16 <- speech16[speech16 != ""]
ind <- regexpr("Applause", speech16)
sum(attr(ind,"match.length") > 1)
bag10 <- strsplit(clean10, " ")
# Strip out all the punctuation, numbers, and carriage returns
clean10 <- gsub("[[:punct:]]", "",speech10)
clean10 <- gsub("[[:digit:]]", "",clean10)
clean10 <- gsub("[^[:graph:]]", " ",clean10)
#convert into bag of words (take all the words and create one giant vector for them)
bag10 <- strsplit(clean10, " ")
bag10 <- tolower(trimws(unlist(bag10)))
#Count the number of times a word shows up
counts10 <- aggregate(bag10, by = list(bag10), FUN = length)
colnames(counts10) <- c("word", "freq")
# Create a column that represents the length of the word in the word column
counts10$len <- nchar(as.character(counts10$word))
counts10 <- counts10[counts10$len > 2,]
dim(counts10)
counts10 <- counts10[order(-counts10$freq),]
head(counts10, 10)
#2016 - do same
clean16 <- gsub("[[:punct:]]", "",speech16)
clean16 <- gsub("[[:digit:]]", "",clean16)
clean16 <- gsub("[^[:graph:]]", " ",clean16)
bag16 <- strsplit(clean16, " ")
bag16 <- tolower(trimws(unlist(bag16)))
counts16 <- aggregate(bag16, by = list(bag16), FUN = length)
colnames(counts16) <- c("word","freq")
counts16$len <- nchar(as.character(counts16$word))
counts16 <- counts16[counts16$len > 2,]
counts16 <- counts16[order(-counts16$freq),]
head(counts16,10)
#2016 - do same
clean16 <- gsub("[[:punct:]]", "",speech16)
clean16 <- gsub("[[:digit:]]", "",clean16)
clean16 <- gsub("[^[:graph:]]", " ",clean16)
bag16 <- strsplit(clean16, " ")
bag16 <- tolower(trimws(unlist(bag16)))
counts16 <- aggregate(bag16, by = list(bag16), FUN = length)
colnames(counts16) <- c("word","freq")
counts16$len <- nchar(as.character(counts16$word))
counts16 <- counts16[counts16$len > 2,]
counts16 <- counts16[order(-counts16$freq),]
head(counts16,10)
stoplist <- digIt("stopwords")
stoplist <- as.vector(stoplist[,1])
library(stopwords)
stoplist <- stopwords("en", source = "snowball")
stoplist <- as.vector(stoplist[,1])
#Remove stopwords
counts10 <- counts10[!(counts10$word %in% stoplist),]
counts16 <- counts16[!(counts16$word %in% stoplist),]
head(counts10,10)
head(counts16,10)
##TFIDF - Term Frequency Inverse Document Frequency
#TF =  (# of times term t appears in a document) / (Total number of terms in the document).
#IDF = exp(# of documents / # documents with term t in it)
master <- merge(counts10, counts16, by = "word", all.x = T, all.y = T)
colnames(master) <- c("word", "freq10", "len10", "freq16","len16")
master[is.na(master)]<-0
master <- merge(counts10, counts16, by = "word", all.x = T, all.y = T)
colnames(master) <- c("word", "freq10", "len10", "freq16","len16")
master[is.na(master)]<-0
#Calculate components - Derive each component for TFIDF
master$tf10 <- master$freq10 / sum(master$freq10)
master$tf16 <- master$freq16 / sum(master$freq16)
master$docs_term <- (master$freq10 > 0) + (master$freq16 > 0)
master$idf <- exp(2/master$docs_term)
master$tfidf10 <- master$tf10 * master$idf
master$tfidf16 <- master$tf16 * master$idf
View(master)
master <- master[, c(1,2,4,10,11)]
library(wordcloud)
set.seed(123)
#Set up plot canvas for two sets
par(mfrow=c(2,1))
master <- master[order(-master$tfidf10),]
wordcloud(master$word[2:100], master$tfidf10[2:100], rot.per = 0)
par(mar=c(1,1,1,1))
master <- master[order(-master$tfidf10),]
wordcloud(master$word[2:100], master$tfidf10[2:100], rot.per = 0)
#Set up plot canvas for two sets
par(mfrow=c(2,1))
master <- master[order(-master$tfidf10),]
wordcloud(master$word[2:100], master$tfidf10[2:100], rot.per = 0)
master <- master[order(-master$tfidf16),]
wordcloud(master$word[2:100], master$tfidf16[2:100], rot.per = 0)
#Set up plot canvas for two sets
par(mfrow=c(2,1))
master <- master[order(-master$tfidf10),]
wordcloud(master$word[2:100], master$tfidf10[2:100], rot.per = 0)
master <- master[order(-master$tfidf16),]
wordcloud(master$word[2:100], master$tfidf16[2:100], rot.per = 0)
# Redownload and try all for yourself.
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-02/data")
master <- data.frame()
years <- seq(2010,2016,1)
for(k in years){
temp <- readLines(paste0("sotu_",k,".txt"))
temp <- temp[temp!=""]
temp <- gsub("[[:punct:]]","",temp)
temp <- gsub("[[:digit:]]","",temp)
temp <- gsub("[^[:graph:]]"," ",temp)
#convert into bag of words
bag <- strsplit(temp," ")
bag <- tolower(trimws(unlist(bag)))
#Loop through
master <- data.frame()
years <- seq(2010,2016,1)
for(k in years){
temp <- readLines(paste0("sotu_",k,".txt"))
temp <- temp[temp!=""]
temp <- gsub("[[:punct:]]","",temp)
temp <- gsub("[[:digit:]]","",temp)
temp <- gsub("[^[:graph:]]"," ",temp)
#convert into bag of words
bag <- strsplit(temp," ")
bag <- tolower(trimws(unlist(bag)))
#Count the number of times a word shows up
counts <- aggregate(bag, by=list(bag), FUN=length)
colnames(counts) <- c("word","freq")
counts$year <- k
master <- rbind(master,counts)
}
master <- master[nchar(master$word) > 2,]
library(rvest)
stop1 <- read_html("http://www.lextek.com/manuals/onix/stopwords1.html")
stopwords <- stop1 %>%
html_nodes("pre") %>%
html_text()
stoplist <- unlist(strsplit(stopwords,"\n"))
stoplist <- stoplist[stoplist!="" & nchar(stoplist)>1]
stoplist <- stoplist[4:length(stoplist)]
#Remove stopwrods
master <- master[!(master$word %in% stoplist),]
wide <- reshape(master,
timevar = "year",
idvar = "word",
direction = "wide")
wide[is.na(wide)] <- 0
wide$tf <- 0
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]]/sum(wide[[paste0("freq.",k)]])
wide$tf <- wide$tf + (wide[[paste0("freq.",k)]] > 0)*1
}
set.seed(80)
par(mfrow=c(2,4))
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]] * exp(length(years)/wide$tf)
wide <- wide[order(-wide[[paste0("freq.",k)]]),]
wordcloud(wide$word[2:100], wide[[paste0("freq.",k)]][2:100], max.words = 100, rot.per = 0)
}
#Re-org data
master <- master[,c(1,2,4,10,11)]
library(wordcloud)
set.seed(50)
temp <- wide[wide$tf == 7,]
temp$sum <- rowSums(temp[,2:8])
wordcloud(temp$word, temp$sum , rot.per = 0, max.words = 100, main="Title")
set.seed(50)
temp <- wide[wide$tf == 7,]
temp$sum <- rowSums(temp[,2:8])
wordcloud(temp$word, temp$sum , rot.per = 0, max.words = 100, main="Title")
temp <- wide[wide$tf == 7,]
wide$tf <- 0
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]]/sum(wide[[paste0("freq.",k)]])
wide$tf <- wide$tf + (wide[[paste0("freq.",k)]] > 0)*1
}
set.seed(80)
par(mfrow=c(2,4))
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]] * exp(length(years)/wide$tf)
wide <- wide[order(-wide[[paste0("freq.",k)]]),]
wordcloud(wide$word[2:100], wide[[paste0("freq.",k)]][2:100], max.words = 100, rot.per = 0)
}
wide$tf <- 0
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]]/sum(wide[[paste0("freq.",k)]])
wide$tf <- wide$tf + (wide[[paste0("freq.",k)]] > 0)*1
}
set.seed(80)
par(mfrow=c(2,4))
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]] * exp(length(years)/wide$tf)
wide <- wide[order(-wide[[paste0("freq.",k)]]),]
wordcloud(wide$word[2:100], wide[[paste0("freq.",k)]][2:100], max.words = 100, rot.per = 0)
}
#Re-org data
master <- master[,c(1,2,4,10,11)]
#Remove stopwrods
master <- master[!(master$word %in% stoplist),]
wide <- reshape(master,
timevar = "year",
idvar = "word",
direction = "wide")
wide[is.na(wide)] <- 0
##TFIDF - Term Frequency Inverse Document Frequency
#TF =  (# of times term t appears in a document) / (Total number of terms in the document).
#IDF = exp(# of documents / # documents with term t in it)
wide$tf <- 0
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]]/sum(wide[[paste0("freq.",k)]])
wide$tf <- wide$tf + (wide[[paste0("freq.",k)]] > 0)*1
}
set.seed(80)
par(mfrow=c(2,4))
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]] * exp(length(years)/wide$tf)
wide <- wide[order(-wide[[paste0("freq.",k)]]),]
wordcloud(wide$word[2:100], wide[[paste0("freq.",k)]][2:100], max.words = 100, rot.per = 0)
}
#Re-org data
master <- master[,c(1,2,4,10,11)]
stopwords <- stop1 %>%
html_nodes("pre") %>%
html_text()
stoplist <- unlist(strsplit(stopwords,"\n"))
stoplist <- stoplist[stoplist!="" & nchar(stoplist)>1]
stoplist <- stoplist[4:length(stoplist)]
#Set your code to your data directory
setwd("/Users/chris/Documents/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro/lecture-02/data")
#Loop through
master <- data.frame()
years <- seq(2010,2016,1)
for(k in years){
temp <- readLines(paste0("sotu_",k,".txt"))
temp <- temp[temp!=""]
temp <- gsub("[[:punct:]]","",temp)
temp <- gsub("[[:digit:]]","",temp)
temp <- gsub("[^[:graph:]]"," ",temp)
#convert into bag of words
bag <- strsplit(temp," ")
bag <- tolower(trimws(unlist(bag)))
#Count the number of times a word shows up
counts <- aggregate(bag, by=list(bag), FUN=length)
colnames(counts) <- c("word","freq")
counts$year <- k
master <- rbind(master,counts)
}
master <- master[nchar(master$word) > 2,]
#Remove stopwords
library(rvest)
#Get stopwords
stop1 <- read_html("http://www.lextek.com/manuals/onix/stopwords1.html")
stopwords <- stop1 %>%
html_nodes("pre") %>%
html_text()
stoplist <- unlist(strsplit(stopwords,"\n"))
stoplist <- stoplist[stoplist!="" & nchar(stoplist)>1]
stoplist <- stoplist[4:length(stoplist)]
#Remove stopwrods
master <- master[!(master$word %in% stoplist),]
wide <- reshape(master,
timevar = "year",
idvar = "word",
direction = "wide")
wide[is.na(wide)] <- 0
##TFIDF - Term Frequency Inverse Document Frequency
#TF =  (# of times term t appears in a document) / (Total number of terms in the document).
#IDF = exp(# of documents / # documents with term t in it)
wide$tf <- 0
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]]/sum(wide[[paste0("freq.",k)]])
wide$tf <- wide$tf + (wide[[paste0("freq.",k)]] > 0)*1
}
set.seed(80)
par(mfrow=c(2,4))
for(k in years){
wide[[paste0("freq.",k)]] <- wide[[paste0("freq.",k)]] * exp(length(years)/wide$tf)
wide <- wide[order(-wide[[paste0("freq.",k)]]),]
wordcloud(wide$word[2:100], wide[[paste0("freq.",k)]][2:100], max.words = 100, rot.per = 0)
}
#Re-org data
master <- master[,c(1,2,4,10,11)]
#Load wordcloud library
library(wordcloud)
set.seed(50)
temp <- wide[wide$tf == 7,]
temp$sum <- rowSums(temp[,2:8])
wordcloud(temp$word, temp$sum , rot.per = 0, max.words = 100, main="Title")
#Create a hypothetical data frame
df <- data.frame( id = seq(1,3,1),
name = c("George Costanza et al. v. Fire Dept",
"Elaine Benes vs. Health Dept.",
"Umbrella Corp. versus Health Dept."),
amount = c("$100k","10,000","No Payout"),
comments = c("Plaintiff keeps angrily referring to Festivus and Toms Diner. He also saw 1 protestor.",
"Expressed discontent about food at Tom's Diner",
"10 protestors seen having food near Toms' Diner"))
##Use gsub() to replace certain characters
df$amount <- gsub("[[:punct:]]","",df$amount)
df$amount <- gsub("k","000",df$amount)
df$amount <- gsub("No Payout",NA,df$amount)
df$amount <- as.numeric(df$amount)
##Standardize "versus","vs.", "v."
#Turn into characters
df$name <- as.character(df$name)
df$name <- gsub("vs\\.|v\\.", "versus",df$name)
#Extract plaintiff and defendent, add to data frame
b <- strsplit(df$name,"versus")
temp <- matrix(unlist(b), nrow=nrow(df), byrow=T)
colnames(temp) <- c("plaintiff","defendent")
df <- cbind(temp, df)
##Extract new features from comments
#replace punctuation with blanks, turn into lower case
df$comments <- gsub("[[:punct:]]","",df$comments)
df$comments <- tolower(df$comments)
#Create binary for each toms diner and food
df$bin_tomsdiner[grep("toms diner", df$comments)] <- 1
df$bin_food[grep("food", df$comments)] <- 1
#extract number of protestors
regmatches(df$comments[3], regexpr("\\d+ protestor+",df$comments[3]))
found <- regexpr("\\d+ protestor+",df$comments)
out <- rep(NA,length(found))
out[found!=-1] <- regmatches(df$comments, found)
out
