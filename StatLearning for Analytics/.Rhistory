perm <- function(n, k) {
for(i in 1:k){
q2 <- permutations(n=k, r=n, repeats.allowed=T)[which(permutations(n=k, r=n, repeats.allowed=T)[,n] != permutations(n=k, r=n, repeats.allowed=T)[,n+1]),]
nrow(q2)
}
}
perm(347, 829)
# install.packages('topicmodels')
library(topicmodels)
# install.packages('dbscan')
library(dbscan)
# install.packages('RnavGraphImageData')
library(RnavGraphImageData)
library(deldir)
set.seed(999)
makespiral = function(s,r1,r2,theta1,dtheta){
r = r1 + s*(r2-r1)
theta = theta1 + s*dtheta
return(c(r*cos(theta),r*sin(theta)))
}
imageoliv = function(x){
y = matrix(x,ncol = 64)
y = t(y[seq(64,1,by = -1),])
image(y,col = gray((0:255)/256))
}
makelinear = function(N,dist, makeplot = T){
x=matrix(rnorm(4*N),2*N,2)
y=rep(c(-1,1),c(N,N))
x[y==1,]=x[y==1,] + dist
if(makeplot){plot(x,col=y+3,pch=19)}
return(list(x=x,y=y))
}
A = makelinear(100, dist = 4.5, makeplot = F)
x = A$x
y = A$y
linear.data1 = data.frame(x,y=as.factor(y))
plot(x[,1],x[,2], col = y+3)
N = 200 # number of points
X = matrix(rnorm(2*N), ncol = 2)
plot(X[,1],X[,2])
km.out=kmeans(X,5,nstart=15)
names(km.out)
km.out$totss
km.out$withinss
km.out$betweenss
sum(km.out$withinss) + km.out$betweenss == km.out$totss
set.seed(129)
km.out1=kmeans(X,5,nstart=1)
km.out1$betweenss
km.out$tot.withinss
km.out$size
km.out$iter
par(mfrow = c(1,2))
boundaries = deldir(km.out$centers[,1], km.out$centers[,2])
plot(boundaries, wlines = 'tess', ylim = c(-2,2),xlim = c(-2,2), lwd = 3)
plot(boundaries, wlines = 'tess', ylim = c(-2,2),xlim = c(-2,2), lwd = 3)
points(X,col=km.out$cluster,pch=19,cex = .7)
par(mfrow = c(1,2))
clust.complete=hclust(dist(X),method="complete")
plot(clust.complete, cex = .2)
clust.cut=cutree(clust.complete,5)
plot(X[,1],X[,2],col = clust.cut, cex = .5, lwd = 3)
par(mfrow = c(1,2))
clust = hclust(dist(X),method="single")
plot(clust, cex = .2)
clust.cut=cutree(clust,5)
plot(X[,1],X[,2],col = clust.cut, cex = .5, lwd = 3)
par(mfrow = c(1,2))
clust = hclust(dist(X),method="average")
plot(clust, cex = .2)
clust.cut=cutree(clust,5)
plot(X[,1],X[,2],col = clust.cut, cex = .5, lwd = 3)
N = 50 # nmake N points per spiral arm
k = 4 # make k arms
r1 = 1
r2 = 3
dtheta = 2*pi
myspiral = data.frame(x=rep(NA,k*N), y = rep(NA,k*N),id = rep(1:k, each = N))
ss = seq(0,1,length = N)
for (j in 1:N){
for (m in 1:k){
myspiral[j + (m-1)*N,1:2] <- makespiral(ss[j],r1,r2,2*pi*m/k,dtheta)
}
}
plot(y ~ x, data = myspiral,col = myspiral$id,lwd = 3, pch = 19)
X = as.matrix(myspiral[,1:2])
kn = 4 # number of clusters
km.out=kmeans(X,kn,nstart=15)
plot(X,col=km.out$cluster,cex=2,pch=1,lwd=2)
clust = hclust(dist(X),method="complete")
clust.cut=cutree(clust,4)
plot(y ~ x, data = myspiral, col = clust.cut, cex = 2, lwd = 2)
points(y ~ x, data = myspiral, col = myspiral$id, pch = 19)
clust.single=hclust(dist(X),method="single")
clust.cut=cutree(clust.single,4)
plot(y ~ x, data = myspiral, col = clust.cut, lwd = 2)
par(mfrow = c(2,2))
data(faces)
head(faces[,1])
imageoliv(faces[,1])
imageoliv(faces[,2])
imageoliv(faces[,301])
imageoliv(faces[,350])
par(mfrow = c(1,3))
X = t(as.matrix(faces))
km.faces = kmeans(X, 3,nstart = 10)
imageoliv(km.faces$centers[1,])
imageoliv(km.faces$centers[2,])
imageoliv(km.faces$centers[3,])
par(mfrow = c(1,3))
X1 = X[km.faces$cluster == 2,]
index = sample(length(X1[,1]),6,replace = F)
for (j in 1:6){
imageoliv(X1[index[j],])
}
par(mfrow = c(1,2))
clust.faces  = hclust(dist(X),method="complete")
head(clust.faces$merge)
clust.faces$merge[200:205,]
imageoliv(faces[,-clust.faces$merge[1,1]])
imageoliv(faces[,-clust.faces$merge[1,2]])
imageoliv(faces[,-clust.faces$merge[2,1]])
imageoliv(faces[,-clust.faces$merge[2,2]])
imageoliv(faces[,-clust.faces$merge[3,1]])
imageoliv(faces[,-clust.faces$merge[3,2]])
imageoliv(faces[,7])
imageoliv(faces[,9])
dbscan_clust <- dbscan(linear.data1[,c(1,2)], eps=1)
plot(linear.data1[,c(1,2)], col=dbscan_clust$cluster+1L)
hullplot(linear.data1[,c(1,2)], dbscan_clust)
data("AssociatedPress")
AssociatedPress
names(AssociatedPress)
# these are the "terms" in the vocabulary
head(AssociatedPress$dimnames[[2]],50)
ap_lda <- LDA(AssociatedPress, k = 4, control = list(seed = 1234))
rm(data, r1, r2, r3, r4)
require(knitr)
require(pROC)
require(ROCR)
require(dbscan)
require(MASS)
require(e1071)
require(ISLR)
require(mlbench)
require(caret)
require(dplyr)
require(readxl)
# install.packages('ellipse')
library(ellipse)
# To be clean, even though we don't need to submit this, go back and delete unused packages
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/Internship Coworking Opportunities/Data Incubator Challenge")
num <- read.delim("2019q1/num.txt", stringsAsFactors = FALSE)
sub <- read.delim("2019q1/sub.txt", stringsAsFactors = FALSE)
pre <- read.delim("2019q1/pre.txt", stringsAsFactors = FALSE)
tag <- read.delim("2019q1/tag.txt", stringsAsFactors = FALSE)
unique(pre$stmt)
# So we want to link with adsh and then subset to end up with just "IS", "CF", and "BS"
unique(pre$line)
unique(pre$report)
# The above two might also be able to help us better align them
###### Sub - adsh, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks
# Subset to adsh, name, sic, countryba, stprba, cityba, zipba, countryma, stprma, cityma, zipma, countryinc, stprinc, ein, former, changed, fye, form, period, fy, fp, filed, prevrpt, detail
# Check on missingness with business address and mailing address - If missing business adress, fill with mailinng address
# sapply(sub2, function(x) sum(is.na(x)))
sub2 <- sub[,c(1, 3:8, 18:22, 25:30, 32:33)]
# Do something with fiscal year subsetting
unique(sub$fp) # First subset to just those with FY
sub2 <- sub2[sub2$fp == "FY",]
unique(sub2$fy) # This shows only 2015-2019 and then some weird 1218, so going to change that one to 2018.
sub2$fy[sub2$fy == 1218] <- c(2018)
# One possibility is to subset to just 10-K. This would be cleanest and most simple at least for initial final round interview data science fellowship purposes. We do this here.
sub2 <- sub2[sub2$form == "10-K",]
# Weâ€™d still end up with 3890 out of 4351 -> Think about it
# So final has 3890, which each represent an Edgar submission.
#
# dfmerge <- left_join(dfmerge2, pre, by=c('adsh'), match='all')
###### Num - adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote
# Subset to adsh, tag, version, ddate, uom, value
# Look further into version. Will it help me to bridge different tags that represent same thing?
num2 <- num[,c(1:3, 5:8)]
unique(num2$qtrs)
# Make sure all tags are same case
num2$tag <- tolower(num2$tag)
# What if we restricted only to those referencing Gaap?
unique(num$version)
nrow(num2[num2$version == "us-gaap/2018" | num2$version == "us-gaap/2017",])
# We'd end up with 1,685,549 out of 2,039,577, but not sure how many companies that consists of since the rows are tags here.
length(unique(num2$tag))
# There are 122,815 total unique tags
# We have 5711 unique here because we haven't subset yet. \
length(unique(num2$tag))
length(unique(num2$adsh))
dfmerge1 <- left_join(num2, sub2, by=c('adsh'))
dfmerge1 <- dfmerge1[!is.na(dfmerge1$name),]
# Subset out those that did not have match in sub2 (i.e. were not associated with 10-K submission)
# We are left with 1600551
length(unique(dfmerge1$adsh))
###### Pre - adsh, report, line, stmt, inpth, rfile, tag, version, plabel
# Subset to IS, CF, and BS
pre2 <- pre[pre$stmt == "IS" | pre$stmt == "CF" | pre$stmt == "BS",]
# Subset to adsh, report, line, stmt, inpth, tag, version, plabel
pre2 <- pre2[,-c(6, 10)]
unique(pre$report)
# Would be good to know eventually when each report number here corresponds to
unique(pre$line)
# Would be good to know eventually when each line number here corresponds to
length(unique(pre$tag))
# Now there are 64,174, so there seem to be discrepencies across
length(uniqu)
###### Pre - adsh, report, line, stmt, inpth, rfile, tag, version, plabel
# Subset to IS, CF, and BS
pre2 <- pre[pre$stmt == "IS" | pre$stmt == "CF" | pre$stmt == "BS",]
# Subset to adsh, report, line, stmt, inpth, tag, version, plabel
pre2 <- pre2[,-c(6, 10)]
# unique(pre$report)
# Would be good to know eventually when each report number here corresponds to
# unique(pre$line)
# Would be good to know eventually when each line number here corresponds to
length(unique(pre2$tag))
# Now there are 64,174, so there seem to be discrepencies across
length(unique(dfmerge1$adsh))
length(unique(dfmerge1$tag))
length(unique(pre2$adsh))
###### Pre - adsh, report, line, stmt, inpth, rfile, tag, version, plabel
# Subset to IS, CF, and BS
pre2 <- pre[pre$stmt == "IS" | pre$stmt == "CF" | pre$stmt == "BS",]
# Subset to adsh, report, line, stmt, inpth, tag, version, plabel
pre2 <- pre2[,-c(6, 10)]
# Make sure all tags are same case
pre2$tag <- tolower(pre2$tag)
# unique(pre$report)
# Would be good to know eventually when each report number here corresponds to
# unique(pre$line)
# Would be good to know eventually when each line number here corresponds to
length(unique(pre2$tag))
# There are 46,233 unique tags compared to 96,271 in dfmerge1
length(unique(pre2$adsh))
# There are 5,721 unique tags compared to 96,271 in dfmerge1
###### Pre - adsh, report, line, stmt, inpth, rfile, tag, version, plabel
# Subset to IS, CF, and BS
pre2 <- pre[pre$stmt == "IS" | pre$stmt == "CF" | pre$stmt == "BS",]
# Subset to adsh, report, line, stmt, inpth, tag, version, plabel
pre2 <- pre2[,-c(6, 10)]
# Make sure all tags are same case
pre2$tag <- tolower(pre2$tag)
# unique(pre$report)
# Would be good to know eventually when each report number here corresponds to
# unique(pre$line)
# Would be good to know eventually when each line number here corresponds to
length(unique(pre2$tag))
# There are 46,233 unique tags compared to 96,271 in dfmerge1
length(unique(pre2$adsh))
# There are 5,721 unique adsh compared to 3,890 in dfmerge1
# Subset pre ahead of time to save room for merge
pre2 <- pre2[pre2$adsh %in% dfmerge1$adsh, ]
dfmerge2 <- left_join(dfmerge1, pre2, by=c('adsh'))
rm(data, r1, r2, r3, r4)
require(knitr)
require(pROC)
require(ROCR)
require(dbscan)
require(MASS)
require(e1071)
require(ISLR)
require(mlbench)
require(caret)
require(dplyr)
require(readxl)
# install.packages('ellipse')
library(ellipse)
# To be clean, even though we don't need to submit this, go back and delete unused packages
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="/Users/chris/Documents/GeorgetownMPPMSFS/Internship Coworking Opportunities/Data Incubator Challenge")
num <- read.delim("2019q1/num.txt", stringsAsFactors = FALSE)
sub <- read.delim("2019q1/sub.txt", stringsAsFactors = FALSE)
pre <- read.delim("2019q1/pre.txt", stringsAsFactors = FALSE)
tag <- read.delim("2019q1/tag.txt", stringsAsFactors = FALSE)
#unique(pre$stmt)
# So we want to link with adsh and then subset to end up with just "IS", "CF", and "BS"
#unique(pre$line)
#unique(pre$report)
# The above two might also be able to help us better align them
###### Sub - adsh, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks
# Subset to adsh, name, sic, countryba, stprba, cityba, zipba, countryma, stprma, cityma, zipma, countryinc, stprinc, ein, former, changed, fye, form, period, fy, fp, filed, prevrpt, detail
# Check on missingness with business address and mailing address - If missing business adress, fill with mailinng address
# sapply(sub2, function(x) sum(is.na(x)))
sub2 <- sub[,c(1, 3:8, 18:22, 25:30, 32:33)]
# Do something with fiscal year subsetting
unique(sub$fp) # First subset to just those with FY
sub2 <- sub2[sub2$fp == "FY",]
unique(sub2$fy) # This shows only 2015-2019 and then some weird 1218, so going to change that one to 2018.
sub2$fy[sub2$fy == 1218] <- c(2018)
# One possibility is to subset to just 10-K. This would be cleanest and most simple at least for initial final round interview data science fellowship purposes. We do this here.
sub2 <- sub2[sub2$form == "10-K",]
# Weâ€™d still end up with 3890 out of 4351 -> Think about it
# So final has 3890, which each represent an Edgar submission.
#
# dfmerge <- left_join(dfmerge2, pre, by=c('adsh'), match='all')
###### Num - adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote
# Subset to adsh, tag, version, ddate, uom, value
# Look further into version. Will it help me to bridge different tags that represent same thing?
num2 <- num[,c(1:3, 5:8)]
# unique(num2$qtrs)
# Make sure all tags are same case
num2$tag <- tolower(num2$tag)
# What if we restricted only to those referencing Gaap?
# unique(num$version)
nrow(num2[num2$version == "us-gaap/2018" | num2$version == "us-gaap/2017",])
# We'd end up with 1,685,549 out of 2,039,577, but not sure how many companies that consists of since the rows are tags here.
length(unique(num2$tag))
# There are 122,815 total unique tags
length(unique(num2$adsh))
# We have 5711 unique here because we haven't subset yet.
dfmerge1 <- left_join(num2, sub2, by=c('adsh'))
dfmerge1 <- dfmerge1[!is.na(dfmerge1$name),]
# Subset out those that did not have match in sub2 (i.e. were not associated with 10-K submission)
# We are left with 1600551
length(unique(dfmerge1$tag))
length(unique(dfmerge1$adsh))
# We are left with 3890 unique adsh and 96,271 unique tags
rm(num, num2, sub, sub2)
###### Pre - adsh, report, line, stmt, inpth, rfile, tag, version, plabel
# Subset to IS, CF, and BS
pre2 <- pre[pre$stmt == "IS" | pre$stmt == "CF" | pre$stmt == "BS",]
# Subset to adsh, report, line, stmt, inpth, tag, version, plabel
pre2 <- pre2[,-c(6, 10)]
# Make sure all tags are same case
pre2$tag <- tolower(pre2$tag)
# unique(pre$report)
# Would be good to know eventually when each report number here corresponds to
# unique(pre$line)
# Would be good to know eventually when each line number here corresponds to
length(unique(pre2$tag))
# There are 46,233 unique tags compared to 96,271 in dfmerge1
length(unique(pre2$adsh))
# There are 5,721 unique adsh compared to 3,890 in dfmerge1
# Subset pre ahead of time to save room for merge
pre2 <- pre2[pre2$adsh %in% dfmerge1$adsh, ]
# We are left with 421,755 observations
dfmerge2 <- left_join(dfmerge1, pre2, by=c('adsh'))
unique(num2$version)
###### Num - adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote
# Subset to adsh, tag, version, ddate, uom, value
# Look further into version. Will it help me to bridge different tags that represent same thing?
num2 <- num[,c(1:3, 5:8)]
num <- read.delim("2019q1/num.txt", stringsAsFactors = FALSE)
sub <- read.delim("2019q1/sub.txt", stringsAsFactors = FALSE)
pre <- read.delim("2019q1/pre.txt", stringsAsFactors = FALSE)
tag <- read.delim("2019q1/tag.txt", stringsAsFactors = FALSE)
#unique(pre$stmt)
# So we want to link with adsh and then subset to end up with just "IS", "CF", and "BS"
#unique(pre$line)
#unique(pre$report)
# The above two might also be able to help us better align them
###### Sub - adsh, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, baph, countryma, stprma, cityma, zipma, mas1, mas2, countryinc, stprinc, ein, former, changed, afs, wksi, fye, form, period, fy, fp, filed, accepted, prevrpt, detail, instance, nciks, aciks
# Subset to adsh, name, sic, countryba, stprba, cityba, zipba, countryma, stprma, cityma, zipma, countryinc, stprinc, ein, former, changed, fye, form, period, fy, fp, filed, prevrpt, detail
# Check on missingness with business address and mailing address - If missing business adress, fill with mailinng address
# sapply(sub2, function(x) sum(is.na(x)))
sub2 <- sub[,c(1, 3:8, 18:22, 25:30, 32:33)]
# Do something with fiscal year subsetting
unique(sub$fp) # First subset to just those with FY
sub2 <- sub2[sub2$fp == "FY",]
unique(sub2$fy) # This shows only 2015-2019 and then some weird 1218, so going to change that one to 2018.
sub2$fy[sub2$fy == 1218] <- c(2018)
# One possibility is to subset to just 10-K. This would be cleanest and most simple at least for initial final round interview data science fellowship purposes. We do this here.
sub2 <- sub2[sub2$form == "10-K",]
# Weâ€™d still end up with 3890 out of 4351 -> Think about it
# So final has 3890, which each represent an Edgar submission.
#
# dfmerge <- left_join(dfmerge2, pre, by=c('adsh'), match='all')
###### Num - adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote
# Subset to adsh, tag, version, ddate, uom, value
# Look further into version. Will it help me to bridge different tags that represent same thing?
num2 <- num[,c(1:3, 5:8)]
# unique(num2$qtrs)
# Make sure all tags are same case
num2$tag <- tolower(num2$tag)
# What if we restricted only to those referencing Gaap?
num2 <- num2[num2$version == "us-gaap/2018" | num2$version == "us-gaap/2017" | num2$version == "us-gaap/2019" | num2$version == "us-gaap/2016"| num2$version == "us-gaap/2015" | num2$version == "us-gaap/2014" | num2$version == "us-gaap/2013" | num2$version == "us-gaap/2012" | num2$version == "us-gaap/2011" | num2$version == "us-gaap/2010",]
nrow(tag[tag$version == "us-gaap/2017" | tag$version == "us-gaap/2018" | tag$version == "us-gaap/2019" | tag$version == "us-gaap/2016" | tag$version == "us-gaap/2015",])
# We'd end up with 1,685,549 out of 2,039,577, but not sure how many companies that consists of since the rows are tags here.
length(unique(num2$tag))
# There are 122,815 total unique tags
length(unique(num2$adsh))
# We have 5711 unique here because we haven't subset yet.
###### Num - adsh, tag, version, coreg, ddate, qtrs, uom, value, footnote
# Subset to adsh, tag, version, ddate, uom, value
# Look further into version. Will it help me to bridge different tags that represent same thing?
num2 <- num[,c(1:3, 5:8)]
# unique(num2$qtrs)
# Make sure all tags are same case
num2$tag <- tolower(num2$tag)
# What if we restricted only to those referencing Gaap?
num2 <- num2[num2$version == "us-gaap/2018" | num2$version == "us-gaap/2017" | num2$version == "us-gaap/2019" | num2$version == "us-gaap/2016"| num2$version == "us-gaap/2015" | num2$version == "us-gaap/2014" | num2$version == "us-gaap/2013" | num2$version == "us-gaap/2012" | num2$version == "us-gaap/2011" | num2$version == "us-gaap/2010",]
length(unique(num2$tag))
# There are 122,815 total unique tags
length(unique(num2$adsh))
# We have 5711 unique here because we haven't subset yet.
dfmerge1 <- left_join(num2, sub2, by=c('adsh'))
dfmerge1 <- dfmerge1[!is.na(dfmerge1$name),]
dfmerge1 <- left_join(num2, sub2, by=c('adsh'))
setwd("~/GeorgetownMPPMSFS/McCourtMPP/Python Learning Files/Kaggle Microlearning/Machine Learning in Python")
require(knitr)
require(MASS)
require(boot)
require(ISLR)
opts_chunk$set(echo = TRUE)
options(digits = 3)
opts_knit$set(root.dir ="~/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
example2 <- read.csv("example2.csv", header=TRUE)
names(example2) <- c("X","Y","Z")
plot(Y ~ X, data = example2, col = Z+1, lwd = 3, asp = 1)
fit <- glm(Z ~ X + Y , data = example2, family=binomial)
example2$pred = predict(fit, example2, type = "response")
example2$class_pred = predict(fit, example2, type = "response") > .5
# We create a threshold at .5 and get a Boolean vector based on true prediction or false prediction
#accuracy - which rows did we get the class prediction correctly equal to z
sum(example2$class_pred == example2$Z) / nrow(example2)
# confusion matrix
table(example2$Z, example2$class_pred)
# Downside to thinking about accuracy - The 0.9 is an average over both classes. But might we be more interested in the fraction I got right in the red or the fraction I got right in the red?
# So we need to think about why you are building the model.
# For example, if I am trying to detect malware and the wrong is a false positive, that's not a big deal because I said it was malware, and you checked and it wasn't. But false negative means we didn't catch malware that was real malware and now we're fucked
# With medicine too, false negative is a bit worse than false positive.
# Confusion matrix
#        T
#    | 0 | 1 |
#    ---------
#    0 |TN|FN|
# P ----------
#   1 |FP|TP|
#   ----------
#
# I can change the threshold if I am really unncomfortable with false positives? Increase threshold because it becomes harder to get to a positive prediction. We have to be pushed in a farther position in our decision boundary space to get over the threshold. This would be a risk of more false negatives though (we'll miss stuff).
# But he pointed to bottom left in output as False Negative and Top Right as False Positive, so be careful!
table(example2$Z, example2$pred > .95)
table(example2$Z, example2$pred > .1)
confusion_mat <- function(df, threshold){
table(df$Z , df$pred > threshold)
}
# Pulling out TP and FP numbers from the confusion matrices and calculating the TP and FP rate. FP = (1 Fales/0) in terms of the confusion. Also known as sensitivity and 1 - specificity, which is what you need to make a ROC curve
# So this might be actually the same as F1!!!
tp_fp = function(mydf,p){
mytable = table(mydf$Z , mydf$pred > p)
x = c(mytable[2,2]/sum(mytable[2,]), mytable[1,2]/sum(mytable[1,]))
names(x) <- c("trueP", "falseP")
return(x)
}
trueP <- falseP <- rep(NA,100)
thresholds <- seq(0,1,length.out=500)
for (j in 2:(length(thresholds)-1)){
x <- tp_fp(example2, thresholds[j])
trueP[j] <- x[1]
falseP[j] <- x[2]
}
plot(falseP, trueP, type='l')
# For any false positive rate youo are willing to tolerate, what true positive rate can your model get?
# The model that lies on a diagonal is doing a really bad job. The model that hangs a right angle from bottom to top towards right is perfect.
# Can summarize the ROC curve with the AOC (Area Under the Curve) -> Integral under the curve
# Worst AUC you can ever have is half. Best you can have is 1. So it will be between 0.5 and 1
load("mnist68.RData")
dim(mnist68)
head(mnist68$labels)
View(mnist68)
load("mnist68.RData")
dim(mnist68)
head(mnist68$labels)
plot_digit <- function(j){
arr784 <- as.numeric(mnist68[j,1:784])
col=gray(12:1/12)
image(matrix(arr784, nrow=28)[,28:1], col=col,
main = paste("this is a ",mnist68$labels[j]))
}
plot_digit(9594)
plot_digit(9594)
plot_digit(9591)
plot_digit(9594)
library(ISLR)
head(Auto)
library(nnet)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- 5 +  2*x1 + -3*x2 + rnorm(100)
plot(x1,y)
df = data.frame(Y=y, X1 = x1, X2 = x2)
linear_fit <- lm(Y ~ X1 + X2, data=df)
summary(linear_fit)
nn_fit <- nnet(Y ~ X1 + X2, data=df, size=2, linout = TRUE)
summary(nn_fit)
myfunc = function(x){x*sin(x)} # Define the function
xx <- seq(-20,20,by=.1) # vector of x variables for plotting
mydf.test = data.frame(x=xx, y = myfunc(xx))
plot(y ~x, data = mydf.test, type = 'l')
# Now make data to train the network on
x <- runif(20, min = -20, max = 20 )
mydf.train = data.frame(x = x, y = myfunc(x))
points(y ~x, data = mydf.train, lwd = 2, col = 2)
# Train network and make predictions
# Note linout = T
net1 <- nnet(y ~x, data = mydf.train, size = 18, decay = .001, maxit = 2000, linout = T)
pred <- predict(net1, newdata = mydf.test, type = "r")
# Make a few plots of predictions
lines(xx,pred, col = 2)
load("mnist68.RData")
load("mnist68.RData")
setwd("~/GeorgetownMPPMSFS/McCourtMPP/Semester 6 Spring 2019/StatLearning for Analytics")
load("mnist68.RData")
load("mnist68.RData")
