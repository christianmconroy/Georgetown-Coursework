{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Day 1: Introduction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Create a \"Client\" object\nclient = bigquery.Client()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all the tables in the \"hacker_news\" dataset-Tell me all of the spreadsheets in this specific dataset)\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are four!)\nfor table in tables:  \n    print(table.table_id)\n# This gave us all the tables we could run queries against","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"full\" table (changed in video to comments)\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n# We named full \"table\" above (changed to comments in video)\ntable.schema","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview the first five lines of the \"full\" table\n# This is like the head command in pandas or R\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview the first five entries in the \"by\" column of the \"full\" table (later changed to comments)\nclient.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 2: Select, From, and Where "},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"openaq\" dataset\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# List all the tables in the \"openaq\" dataset (There is only one)\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:  \n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"global_air_quality\" table (i want a map that tells me how to directly \n# get to this table)\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"global_air_quality\" table (Just like the head command)\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table.schema\n# This is useful as it often will contain the description of the variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\nquery = \"\"\"\n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\n# The triple quotations is a way to comment over multiple lines","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a \"Client\" object\nclient = bigquery.Client()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query\nquery_job = client.query(query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_cities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What five cities have the most measurements? (NOW WE'RE MOVING ON TO REGULAR PANDAS STUFF)\nus_cities.city.value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\n        SELECT city, source_name\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\nquery_job = client.query(query)\n\nus_cities = query_job.to_dataframe()\n\nus_cities.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\nquery_job = client.query(query)\n\nus_cities = query_job.to_dataframe()\n\nus_cities.head()\n\n# Prob won't do this a lot because data on bigquery is very big and there may be limits to pull\n# Bigquery caches recent queries so if you run it multiple times it'll be faster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to get the score column from every row where the type column has value \"job\"\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = \"IN\" \n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n\n# This can be helpful if it's a new dataset and you don't know how much time it will take for your query \n# to run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only run the query if it's less than 100 MB\nONE_HUNDRED_MB = 100*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_HUNDRED_MB)\n\n# Set up the query (will only run if it's less than 100 MB)\nsafe_query_job = client.query(query, job_config=safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_query_job.to_dataframe()\n\n# You can run the query with a little bit of a seatbelt: We say that I only want to run jobs if they \n# are less than 100 megabytes (Remember it is only calculated in bytes)\n\n# Remember that if it's cached and you run it again, it won't count against your bigquery quota \n# You can get an exceeded limit for bytes billed, so be careful with credit card payments obviously","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 3: Groupby, Having, Where"},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"\n\n# id is the unique identifier here \n# We only look at the ones that have more than ten comments\n# We're creating a text string with a valid SQL text string to it, then we hand it to the client, and \n# then the client gives it to the bigquery","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()\n\n# We end up with the hacker news parent posts and the number of comments for each","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Improved version of earlier query, now with aliasing & improved readability\nquery_improved = \"\"\"\n                 SELECT parent, COUNT(1) AS NumPosts\n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY parent\n                 HAVING COUNT(1) > 10\n                 \"\"\"\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()\n\n# You can use Count(1) if you have a column called id, but also just if you have one column with \n# unique identifiers ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_good = \"\"\"\n             SELECT parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_bad = \"\"\"\n            SELECT author, parent, COUNT(id)\n            FROM `bigquery-public-data.hacker_news.comments`\n            GROUP BY parent\n            \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 4: Order by, Extract, Dates"},{"metadata":{},"cell_type":"markdown","source":"Introduction\nSo far, you've learned how to use several SQL clauses. For instance, you know how to use SELECT to pull specific columns from a table, along with WHERE to pull rows that meet specified criteria. You also know how to use aggregate functions like COUNT(), along with GROUP BY to treat multiple rows as a single group.\n\nNow you'll learn how to change the order of your results using the ORDER BY clause, and you'll explore a popular use case by applying ordering to dates. To illustrate what you'll learn in this tutorial, we'll work with a slightly modified version of our familiar pets table."},{"metadata":{},"cell_type":"markdown","source":"### ORDER BY\nORDER BY is usually the last clause in your query, and it sorts the results returned by the rest of your query.\n\nNotice that the rows are not ordered by the ID column. We can quickly remedy this with the query below."},{"metadata":{"trusted":true},"cell_type":"code","source":"## ORDER\n# Can sort by an ID, by another column (Alphabetically for categorical for example)\n# To reverse orders, use DESC at the end\n\n# query = '''\n# SELECT ID Name, Animal\n# FROM 'thequery'\n# ORDER BY Animal DESC\n# '''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dates\nNext, we'll talk about dates, because they come up very frequently in real-world databases. There are two ways that dates can be stored in BigQuery: as a DATE or as a DATETIME."},{"metadata":{"trusted":true},"cell_type":"code","source":"## The DATE format has the year first, then the month, and then the day. It looks like this:\n\n## YYYY-[M]M-[D]D\n\n# YYYY: Four-digit year\n# [M]M: One or two digit month\n# [D]D: One or two digit day\n\n# The DATETIME format is like the date format ... but with time added at the end.\n\n# Can look up standard-sql/date_functions on big query if needed\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EXTRACT\nOften you'll want to look at part of a date, like the year or the day. You can do this with EXTRACT. We'll illustrate this with a slightly different table, called pets_with_date.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# query = \"\"\"\n# SELECT Name, EXTRACT(DAY from Date) AS Day\n# FROM 'queryname'\n# \"\"\"\n\n# query = \"\"\"\n# SELECT Name, EXTRACT(WEEK from Date) AS Week\n# FROM 'queryname'\n# \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"accident_2015\" table\ntable_ref = dataset_ref.table(\"accident_2015\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"accident_2015\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to find out the number of accidents for each day of the week\nquery = \"\"\"\n        SELECT COUNT(consecutive_number) AS num_accidents, \n               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents DESC\n        \"\"\"\n\n# This query selects the count of consecutive number, and extract day of week from time stamp of crash\n# Note that it seems that we're referencing the database and the table in one go in from.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We won't run this query if it is going to take more than this amount of billing!\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 1 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(query, job_config=safe_config)\n# We set up the query above, so all we needed to change was the text in query, which we just did\n# API request - run the query, and convert the results to a pandas DataFrame\naccidents_by_day = query_job.to_dataframe()\n\n# Print the DataFrame\naccidents_by_day\n\n# Sunday is 1, but most accidents are on Saturday (AM drunk driving probs)\n# Safest day of the week looks like Tuesday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 5: AS and WITH"},{"metadata":{},"cell_type":"markdown","source":"### Introduction\nWith all that you've learned, your SQL queries are getting pretty long, which can make them hard understand (and debug).\n\nYou are about to learn how to use AS and WITH to tidy up your queries and make them easier to read.\n\nAlong the way, we'll use the familiar pets table, but now it includes the ages of the animals."},{"metadata":{},"cell_type":"markdown","source":"### AS\nYou learned in an earlier tutorial how to use AS to rename the columns generated by your queries, which is also known as aliasing. This is similar to how Python uses as for aliasing when doing imports like import pandas as pd or import seaborn as sns.\n\nTo use AS in SQL, insert it right after the column you select. Here's an example of a query without an AS clause"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example Without AS\n### query =\"\"\"\n# SELECT animal, COUNT(ID)\n# FROM 'bigqueryname'\n# GROUP BY Animal\n# \"\"\"\n\n# Example With AS\n### query =\"\"\"\n# SELECT animal, COUNT(ID) AS Number \n# FROM 'bigqueryname'\n# GROUP BY Animal\n# \n\n# These queries return the same information, but in the second query the column returned by the \n# COUNT() function will be called Number, rather than the default name of f0__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WITH ... AS\nOn its own, AS is a convenient way to clean up the data returned by your query. It's even more powerful when combined with WITH in what's called a \"common table expression\".\n\nA common table expression (or CTE) is a temporary table that you return within your query. CTEs are helpful for splitting your queries into readable chunks, and you can write queries against them.\n\nFor instance, you might want to use the pets table to ask questions about older animals in particular. So you can start by creating a CTE which only contains information about animals more than five years old like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### query =\"\"\"\n# WITH Seniors AS\n# (SELECT ID, Name\n# FROM 'bigqueryname'\n# WHERE Years_old > 5\n# )\n# \"\"\"\n\n# Note that the above is just a part of a query and it is not complete","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While this incomplete query above won't return anything, it creates a CTE that we can then refer to (as Seniors) while writing the rest of the query.\n\nWe can finish the query by pulling the information that we want from the CTE. The complete query below first creates the CTE, and then returns all of the IDs from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"### query =\"\"\"\n# WITH Seniors AS\n# (SELECT ID, Name\n# FROM 'bigqueryname'\n# WHERE Years_old > 5\n# )\n# SELECT ID \n# FROM Seniors\n\n# This is the full version because now we're actually doing something","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You could do this without a CTE, but if this were the first part of a very long query, removing the CTE would make it much harder to follow.\n\nAlso, it's important to note that CTEs only exist inside the query where you create them, and you can't reference them in later queries. So, any query that uses a CTE is always broken into two parts: (1) first, we create the CTE, and then (2) we write a query that uses the CTE."},{"metadata":{},"cell_type":"markdown","source":"### Example: How many Bitcoin transactions are made per month?\nWe're going to use a CTE to find out how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset.\n\nWe'll investigate the transactions table. Here is a view of the first few rows. (The corresponding code is hidden, but you can un-hide it by clicking on the \""},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"crypto_bitcoin\" dataset\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"transactions\" table\ntable_ref = dataset_ref.table(\"transactions\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"transactions\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the block_timestamp column contains the date of each transaction in DATETIME format, we'll convert these into DATE format using the DATE() command.\n\nWe do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select the number of transactions per date, sorted by date\nquery_with_CTE = \"\"\" \n                 WITH time AS \n                 (\n                     SELECT DATE(block_timestamp) AS trans_date\n                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n                 )\n                 SELECT COUNT(1) AS transactions,\n                        trans_date\n                 FROM time\n                 GROUP BY trans_date\n                 ORDER BY trans_date\n                 \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_with_CTE, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()\n\n# We could technically create more thana one of these CTEs in the same query if we'd want to.\n# The idea is that we could then do joins internally","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since they're returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions_by_date.set_index('trans_date').plot()\n\n# The set index part means that we are sorting by transaction date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. That's an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas"},{"metadata":{},"cell_type":"markdown","source":"# Day 6: Joining Data"},{"metadata":{},"cell_type":"markdown","source":"### Introduction\nYou have the tools to obtain data from a single table in whatever format you want it. But what if the data you want is spread across multiple tables?\n\nThat's where JOIN comes in! JOIN is incredibly important in practical SQL workflows. So let's get started."},{"metadata":{},"cell_type":"markdown","source":"### Example\nWe'll use our imaginary pets table, which has three columns:\n\nID - ID number for the pet\nName - name of the pet\nAnimal - type of animal\nWe'll also add another table, called owners. This table also has three columns:\n\nID - ID number for the owner (different from the ID number for the pet)\nName - name of the owner\nPet_ID - ID number for the pet that belongs to the owner (which matches the ID number for the pet in the pets table)\n\nFor example,\n\nthe pets table shows that Dr. Harris Bonkers is the pet with ID 1.\nThe owners table shows that Aubrey Little is the owner of the pet with ID 1.\nPutting these two facts together, Dr. Harris Bonkers is owned by Aubrey Little.\n\nFortunately, we don't have to do this by hand to figure out which owner goes with which pet. In the next section, you'll learn how to use JOIN to create a new table combining information from the pets and owners tables."},{"metadata":{},"cell_type":"markdown","source":"### JOIN\nUsing JOIN, we can write a query to create a table with just two columns: the name of the pet and the name of the owner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# query =\"\"\"\n# SELECT p.name AS Pet_Name, \n# o.Name AS Owner_Name\n# FROM `bigqueryname.pets` AS p\n#INNER JOIN `bigqueryname.owners` AS o\n#   ON p.ID = o.Pet_ID\n# \"\"\"\n\n# We alias down, but can reference the alias above like we do here. \n# Name is a column in the pets table \n# Inner Join means we only get items that occur in both\n    # Logical OR -> You'll get it if you get it in both of them and not just one\n    # Logical AND -> Get everything from both tables (Outer Join)\n        # Don't do it. You'll blow through your quota if you don't have the safety belt\n    # A guy without a pet would not end up in the merge then\n    # Inner Join is the safest when it comes to how big the resulting dataset is\n# ON tells us which columns to look for to link the two datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example: How many files are covered by each type of software license?\nGitHub is the most popular place to collaborate on software projects. A GitHub repository (or repo) is a collection of files associated with a specific project.\n\nMost repos on GitHub are shared under a specific legal license, which determines the legal restrictions on how they are used. For our example, we're going to look at how many different files have been released under each license.\n\nWe'll work with two tables in the database. The first table is the licenses table, which provides the name of each GitHub repo (in the repo_name column) and its corresponding license. Here's a view of the first five rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"github_repos\" dataset\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"licenses\" table\nlicenses_ref = dataset_ref.table(\"licenses\")\n\n# API request - fetch the table\nlicenses_table = client.get_table(licenses_ref)\n\n# Preview the first five lines of the \"licenses\" table\nclient.list_rows(licenses_table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second table is the sample_files table, which provides, among other information, the GitHub repo that each file belongs to (in the repo_name column). The first several rows of this table are printed below"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"sample_files\" table\nfiles_ref = dataset_ref.table(\"sample_files\")\n\n# API request - fetch the table\nfiles_table = client.get_table(files_ref)\n\n# Preview the first five lines of the \"sample_files\" table\nclient.list_rows(files_table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we write a query that uses information in both tables to determine how many files are released in each license."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to determine the number of files per license, sorted by number of files\nquery = \"\"\"\n        SELECT L.license, COUNT(1) AS number_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n            ON sf.repo_name = L.repo_name\n        GROUP BY L.license\n        ORDER BY number_of_files DESC\n        \"\"\"\n# I can change this to COUNT(sf.id) if I want to do the counting from a specific dataset\n\n# What are we doing here? \n    # We select license from the table we define as L, which is the licenses table in the github_repos\n    # dataset\n    # number_of_files will technically come from sample-files\n    # We do an inner join then\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the DataFrame\nfile_count_by_license","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 7: Machine Learning with SQL"},{"metadata":{},"cell_type":"markdown","source":"This tutorial is an introduction to BigQuery ML. It is based on the official documentation tutorial. In this tutorial, you use the sample Google Analytics sample dataset for BigQuery to create a model that predicts whether a website visitor will make a transaction. For information on the schema of the Analytics dataset, see BigQuery export schema.\n\nThere is also an accopmnying livestream video, which can be found here and a notebook with exercises for you to try out."},{"metadata":{},"cell_type":"markdown","source":"###What is BigQuery ML and when should you use it?\nBigQuery Machine Learning (BQML) is a toolset that allows you to train and serve machine learning models directly in BigQuery. This has several advantages:\n\nYou don't have to read your data into local memory. One question I get a lot is \"how can I train my ML model if my dataset is just too big to fit on my computer?\". You can subsample your dataset, of course, but you can also use tools like BQML that train your model directly in your database.\n\nYou don't have to use multiple languages. Particularly if you're working in a team where most of your teammates don't know Python or R or your preferred language for modelling, working in SQL can make it easier for you to collaborate.\n\nYou can serve your model immediately after it's trained. Because your model is already in the same place as your data, you can make predictions directly from your database. This lets you get around the hassle of cleaning up your code and either putting it intro production or passing it off to your engineering colleagues.\n\nBQML probably won't replace all your modelling tools, but it's a nice quick way to train and serve a model without spending a lot of time moving code or data around."},{"metadata":{},"cell_type":"markdown","source":"Objectives\nIn this tutorial, you will use:\n\nBQML to create a binary logistic regression model using the CREATE MODEL statement\nThe ML.EVALUATE function to evaluate the ML model\nThe ML.PREDICT function to make predictions using the ML model[](http://)"},{"metadata":{},"cell_type":"markdown","source":"### Step one: Setup and create your dataset\nFirst, you'll need to create a BigQuery dataset to store your ML model. You don't actually have to upload any data; the only table in it will be the one with your trained model.\n\nIf you're curious about the client and what it does, check out the first lesson in the Intro SQL course and the accompying video.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set your own project id here\nPROJECT_ID = 'kagglebq-test'\n\n# Create a client instance for your project\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a small array and save it as a csv\nimport numpy\na = numpy.asarray([[1,2,3], [4,5,6], [7,8,9]])\nnumpy.savetxt(\"foo.csv\", a, delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new dataset \nclient.create_dataset('new_dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new table in that dataset ()\nclient.create_table(f\"kagglebq-test.new_dataset.new_table\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Upload CSV file to BigQuery \n# foo.csv is currently in our directory\nfilename = \"foo.csv\"\ndataset_id = \"new_dataset\"\ntable_id = \"new_table\"\n\n# Tell the client everything it needs to know to upload our csv\ndataset_ref = client.dataset(dataset_id)\ntable_ref = dataset_ref.table(table_id)\njob_config = bigquery.LoadJobConfig()\njob_config.source_format = bigquery.SourceFormat.CSV\njob_config.autodetect = True\n\n# Load the CSV into BigQuery\nwith open(filename, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n\njob.result() #Waits for table load to complete\n\n# Results:\nprint(\"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id))\n\n# It's 3 because we made an array with 3 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#query\nquery = f\"\"\" SELECT *\n        FROM `kagglebq-test.new_dataset.new_table`\"\"\"\n\n#Set up the query \nquery_job = client.query(query)\n\n#API request -run the query, and return a pandas DataFrame\ndata = query_job.to_dataframe()\n\n# Remember that tricky business with the back ticks!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above was for the first set up video. The below is for the actual lesson."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set your own project id here\nPROJECT_ID = 'kagglebq-test'\n\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")\ndataset = client.create_dataset('bqml_tutorial', exists_ok=True)\n\nfrom google.cloud.bigquery import magics\nfrom kaggle.gcp import KaggleKernelCredentials\nmagics.context.credentials = KaggleKernelCredentials()\nmagics.context.project = PROJECT_ID\n\n# Previously, we interacted directly with the client. \n# Today, we're using the notebook magic.\n    # You can type sql directly without putting in triple quotes, send to query object, send that to client \n    # Will only work in jupyter\n    # Less flexible but less typing ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this example, we will use the Google Analytics sample dataset to predict whether a website visitor will make a transaction. First, let's take a quick look at the data we'll be using."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a reference to our table\ntable = client.get_table(\"bigquery-public-data.google_analytics_sample.ga_sessions_*\")\n\n# look at five rows from our dataset\nclient.list_rows(table, max_results=5).to_dataframe()\n\n# Google analytics is the dataset and ga.sessions_* is a range of tables\n    # Every table that starts with ga_sessions_\n    # Each day gets a new table in this dataset, so we're getting the table for all days\n    \n# Strux or recrods: JSON file inside a cell \n    # If we look at the schema of the table, we can see the entire defined schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This table looks a little different from some of the ones you've seen in the Intro to SQL course. Notably the columns for \"totals\", \"trafficSource\", \"device\", \"geoNetwork\", \"customDimensions\" and \"hits\" are of a new data type called STRUCT (or RECORD) which is used for nested data structures. We'll learn more about these in the advanced SQL course, but for now all you need to know is:\n\nEach cell in a specific column has the same fields in it. (These are specified in the schema.)\nYou can get the information from a specific field for a whole column using the syntax COLUMN_NAME.FIELD_NAME.\nSo for the \"totals\" column, we could get the \"transactions\" field by using the syntax totals.transactions. (You can see all the fields in a record from the \"totals\" column below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a small sample dataframe\nsample_table = client.list_rows(table, max_results=5).to_dataframe()\n\n# get the first cell in the \"totals\" column\nsample_table.totals[0]\n\n# Column name . the field name we are interested in.\n    # We created a small dataset from the table, and then we looked in the totals column, and got info from first row ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step two: Create your model\nNext, we will create a logistic regression model for classification.\n\nThe standard SQL query uses a CREATE MODEL statement to create and train the model. You can find the documentation for this fuction here.\n\nThe BigQuery Python client library provides a custom magic command so that you don't need to set up the queries yourself. To load the magic commands from the client library, run the following code.\n\nIf you prefer not to use the magic command, you can use client-based way of writing queries covered in the Intro to SQL course. They'll both work the same, it's just a slightly different style of writing code. :)"},{"metadata":{},"cell_type":"markdown","source":"For this problem, we'll be trying to predict transactions from the totals column, so keep this in mind! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext google.cloud.bigquery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"%load_ext is one of the many Jupyter built-in magic commands. See the Jupyter documentation for more information about %load_ext and other magic commands.\n\nThe BigQuery client library provides a cell magic, %%bigquery, which runs a SQL query and returns the results as a Pandas DataFrame. Once you use this command the rest of your cell will be treated as a SQL command. (Note that tab complete won't work for SQL code written in this way.)\n\nHere's the the query that will train our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nCREATE MODEL IF NOT EXISTS `bqml_tutorial.sample_model2`\nOPTIONS(model_type='logistic_reg') AS\nSELECT\n  IF(totals.transactions IS NULL, 0, 1) AS label,\n  IFNULL(device.operatingSystem, \"\") AS os,\n  device.isMobile AS is_mobile,\n  IFNULL(geoNetwork.country, \"\") AS country,\n  IFNULL(totals.pageviews, 0) AS pageviews\nFROM\n  `bigquery-public-data.google_analytics_sample.ga_sessions_*`\nWHERE\n  _TABLE_SUFFIX BETWEEN '20160801' AND '20170630'\n    \n# There is apparently no overwriting, so you can just run this once unless you delete it. \n# Otherwise you just have to rename (that's why it's now sample_model2.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# REFERENCING THE CELL ABOVE\n# First thing we want to do is create our model. \n# If you look in the console on GCP, you'll now see the sample_model. \n# We use Options to say we're training a logistic regression \n    # You could also choose to say which column you want to be your target column (we use the first column here and \n    #call it label - could of specified earlier too)\n    # You can choose your optimization strategy, but a lot is done automatically\n    # You can choose testing training split, but that's done automatically too\n# Then we have actual variables we're using as independent. \n    # If you try to predict a null, you'll get an error obviously, so we had to invert nones into 0s or otherwise 1\n    # We're predicting whether or not there will be a transaction, but the number of transactions \n# We use the ifnull to clean essentially - not from country none, but the country is unknown\n    \n# At the bottom, we say what tables we'll use to train the model \n# Then we say which of the tables we'll actually use \n\n\n# **Tip:** Using ```CREATE OR REPLACE MODEL``` rather than just ```CREATE MODEL``` ensures you don't get an error if you \n# want to run this command again without first deleting the model you've created.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's break down this command a little bit. This is a fairly fancy query, which is nice because it lets you see a lot of the options you have when using BQML.\n\nThis line is writing our model to BigQuery. (The fact that BQML requires write permissions is why you needed to set up your GCP account; the default Kaggle connection doesn't allow you write tables, only query them."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nCREATE MODEL IF NOT EXISTS `bqml_tutorial.sample_model2`","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we're specifying that our model will be logistic regression, so we know it's a classification task. Next we need to actually define our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIONS(model_type='logistic_reg') AS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code under the SELECT clause is where we define the variable we're trying to predict as well as what variables we want to use to predict it.\n\nThe column we alias as \"label\" will be our dependent variable, the thing we're trying to predict. The IF(totals.transactions IS NULL, 0, 1) bit just means that if the values of the transactions field is NULL or None, we'll assign it a value of \"0\", otherwise it will have a value of one. Since we have two labels, BQML will automatically use binary regression for us.\n\nThe other four rows say what information we want to use to predict that label. Here we're information on the device operating system, whether it's a mobile device, the country and the number of pageviews. For the os, country and pageviews, we're assigning appropriate values to null fields. Either making them blank (so we're not saying people are from NULL country!) or setting them to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"SELECT\n  IF(totals.transactions IS NULL, 0, 1) AS label,\n  IFNULL(device.operatingSystem, \"\") AS os,\n  device.isMobile AS is_mobile,\n  IFNULL(geoNetwork.country, \"\") AS country,\n  IFNULL(totals.pageviews, 0) AS pageviews\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The FROM clause specifies the table (in this case more than one table) that we're going to get our data from."},{"metadata":{"trusted":true},"cell_type":"code","source":"FROM\n  `bigquery-public-data.google_analytics_sample.ga_sessions_*`","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally the WHERE clause specifies the range of tables to use to train our model. In this case it looks like a new table is being produced every day, so we'll be using data from between August 1, 2016 and June 30, 2017 to train our model. (You would only need to specify range if you were using some tables out of all the possible tables in your dataset.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"WHERE\n  _TABLE_SUFFIX BETWEEN '20160801' AND '20170630'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The query takes several minutes to complete. After the first iteration is complete, your model (sample_model) appears in the navigation panel of the BigQuery UI. Because the query uses a CREATE MODEL statement to create a table, you do not see query results. The output is an empty string."},{"metadata":{},"cell_type":"markdown","source":"### Step three: Get training statistics\nTo see the results of the model training, you can use the ML.TRAINING_INFO function, or you can view the statistics in the BigQuery UI. In this tutorial, you use the ML.TRAINING_INFO function.\n\nA machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss. This process is called empirical risk minimization.\n\nLoss is the penalty for a bad prediction â€” a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights that have low loss, on average, across all examples.\n\nTo see the model training statistics that were generated when you ran the CREATE MODEL query, run the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nSELECT\n  *\nFROM\n  ML.TRAINING_INFO(MODEL `bqml_tutorial.sample_model`)\nORDER BY iteration \n\n# Because we had set it up in previous iteration (changes to samplemodel2 after to run again)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: Typically, it is not a best practice to use a SELECT * query. Because the model output is a small table, this query does not process a large amount of data. As a result, the cost is minimal.\n\nThe loss column represents the loss metric calculated after the given iteration on the training dataset. Since you performed a logistic regression, this column is the log loss. The eval_loss column is the same loss metric calculated on the holdout dataset (data that is held back from training to validate the model).\n\nAt this point you'll notice that BQML has taken care of some of the common ML decisions for you:\n\nSplitting into training & evaluation datasets to help detect overfitting\nEarly stopping (stopping training when additional iterations would not improve performance on the evaluation set)\nPicking and updating learning rates (starting with a low learning rate and increasing it over time)\nPicking an optimization strategy (batch gradient descent for large datasets with high cardinality, normal equation for small datasets where it would be faster)\nFor more details on the ML.TRAINING_INFO function, see the BQML syntax reference."},{"metadata":{},"cell_type":"markdown","source":"### Step four: Evaluate your model\nAfter creating your model, you evaluate the performance of the classifier using the ML.EVALUATE function.\n\nA classifier is one of a set of enumerated target values for a label. For example, in this tutorial you are using a binary classification model that detects transactions. The two classes are the values in the label column: 0 (no transactions) and not 1 (transaction made).\n\nTo run the ML.EVALUATE query that evaluates the model, run the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nSELECT\n  *\nFROM ML.EVALUATE(MODEL `bqml_tutorial.sample_model`, (\n  SELECT\n    IF(totals.transactions IS NULL, 0, 1) AS label,\n    IFNULL(device.operatingSystem, \"\") AS os,\n    device.isMobile AS is_mobile,\n    IFNULL(geoNetwork.country, \"\") AS country,\n    IFNULL(totals.pageviews, 0) AS pageviews\n  FROM\n    `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20170701' AND '20170801'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While it's helpful to see these metrics, it's also common to plot the ROC curve when evaluating model performance for binary logistic regression. We can do this by using the ML.ROC_CURVE() function.\n\nPro-tip: You can save the output of a bigquery magic cell by putting a variable name to the right of the %%bigquery command. Here I've saved the output of the next cell as the variable \"roc\" ."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery roc\nSELECT\n  *\nFROM\n  ML.ROC_CURVE(MODEL `bqml_tutorial.sample_model`)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out the data that was returned...\nroc.head()\n\n# Super useful.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and plot our ROC curve!\nimport matplotlib.pyplot as plt\n\n# plot the false positive rate by true postive rate (aka recall)\nplt.plot(roc.false_positive_rate, roc.recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step five: Use your model to predict outcomes\nNow that you have evaluated your model, the next step is to use it to predict outcomes. You use your model to predict the number of transactions made by website visitors from each country. And you use it to predict purchases per user.\n\nTo run the query that uses the model to predict the number of transactions by country. (Note that you only need to SELECT the column you're grouping by, not every column you used in your prediction.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nSELECT\n  country,\n  SUM(predicted_label) as total_predicted_purchases\nFROM ML.PREDICT(MODEL `bqml_tutorial.sample_model`, (\n  SELECT\n    IFNULL(device.operatingSystem, \"\") AS os,\n    device.isMobile AS is_mobile,\n    IFNULL(totals.pageviews, 0) AS pageviews,\n    IFNULL(geoNetwork.country, \"\") AS country\n  FROM\n    `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20170701' AND '20170801'))\n  GROUP BY country\n  ORDER BY total_predicted_purchases DESC\n  LIMIT 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We typically end up just repeating the code upon prediction \n# Above, I select country and sum over the predicted labels \n    # I group by country \n    # I order by predicted purchases descending \n    # This is the number of visits that I anticipate will become a transaction\n    # Then I only show the first ten rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next example, you try to predict the number of transactions each website visitor will make. This query is identical to the previous query except for the GROUP BY clause. Here the GROUP BY clause â€” GROUP BY fullVisitorId â€” is used to group the results by visitor ID.\n\nTo run the query that predicts purchases per user:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bigquery\nSELECT\n  fullVisitorId,\n  SUM(predicted_label) as total_predicted_purchases\nFROM ML.PREDICT(MODEL `bqml_tutorial.sample_model`, (\n  SELECT\n    IFNULL(device.operatingSystem, \"\") AS os,\n    device.isMobile AS is_mobile,\n    IFNULL(totals.pageviews, 0) AS pageviews,\n    IFNULL(geoNetwork.country, \"\") AS country,\n    fullVisitorId\n  FROM\n    `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20170701' AND '20170801'))\n  GROUP BY fullVisitorId\n  ORDER BY total_predicted_purchases DESC\n  LIMIT 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Above is the people I think are likely to make purchases\n# If we were doing B2B, this might be the business that I want to reach out to obviously as they are likely to make a \n# purchase","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleanup\nTo avoid cluttering up your Google Cloud Platform account with the models you trained in this tutorial:\n\nYou can delete the project you created.\nOr you can keep the project and delete the dataset."},{"metadata":{},"cell_type":"markdown","source":"# Day 8: Analytics Functions"},{"metadata":{},"cell_type":"markdown","source":"#### Introduction\nIn the Intro to SQL micro-course, you learned how to use aggregate functions, which perform calculations based on sets of rows. In this tutorial, you'll learn how to define analytic functions, which also operate on a set of rows. However, unlike aggregate functions, analytic functions return a (potentially different) value for each row in the original table."},{"metadata":{},"cell_type":"markdown","source":"We'll work with the San Francisco Open Data dataset. We begin by reviewing the first several rows of the bikeshare_trips table. (The corresponding code is hidden, but you can un-hide it by clicking on the \"Code\" button below.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"san_francisco\" dataset\ndataset_ref = client.dataset(\"san_francisco\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"bikeshare_trips\" table\ntable_ref = dataset_ref.table(\"bikeshare_trips\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each row of the table corresponds to a different bike trip, and we can use an analytic function to calculate the cumulative number of trips for each date in 2015"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to count the (cumulative) number of trips per day\nnum_trips_query = \"\"\"\n                  WITH trips_by_day AS\n                  (\n                  SELECT DATE(start_date) AS trip_date,\n                      COUNT(*) as num_trips\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE EXTRACT(YEAR FROM start_date) = 2015\n                  GROUP BY trip_date\n                  )\n                  SELECT *,\n                      SUM(num_trips) \n                          OVER (\n                               ORDER BY trip_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n                               ) AS cumulative_trips\n                      FROM trips_by_day\n                  \"\"\"\n\n# First thing we do is a common table expression (Creating a new table we can refer to in rest of query called trips_by_day)\n    # Select date from start_date, counting all rows as num_trips and group_by trip_date \n    # All rows from same date together and count how many per date \n    # We're only looking at 2015\n# We want cumulative sum of bike trips \n    # We use a windowing function\n        # Not doinng a partition here \n        # We order by trip date\n        # Unbounded preceding and current row means all of previous rows and the current row\n            # We take the sum of all that and call it cumulative_trip\n            # All comes from our CTE\n\n\n# Run the query, and return a pandas DataFrame\nnum_trips_result = client.query(num_trips_query).result().to_dataframe()\nnum_trips_result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next query tracks the stations where each bike began (in start_station_id) and ended (in end_station_id) the day on October 25, 2015."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to track beginning and ending stations on October 25, 2015, for each bike\nstart_end_query = \"\"\"\n                  SELECT bike_number,\n                      TIME(start_date) AS trip_time,\n                      FIRST_VALUE(start_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS first_station_id,\n                      LAST_VALUE(end_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS last_station_id,\n                      start_station_id,\n                      end_station_id\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE DATE(start_date) = '2015-10-25' \n                  \"\"\"\n# We create two windowing functions\n    # Find first value of start station ID over the windowing function where we partition by bike number, order by start_date,\n    # and then look at all rows in partition\n    \n    # Then last value of end station ID, partition by bike number, order by start, but look at last value of start \n    # station id \n    # We ignore start and end for trips that were not the first trip on day when it is 2015/10/25\n\n# Run the query, and return a pandas DataFrame\nstart_end_result = client.query(start_end_query).result().to_dataframe()\nstart_end_result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 9: Joins and Unions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"stories\" table\ntable_ref = dataset_ref.table(\"stories\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The query below pulls information from the stories and comments tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments. We use a LEFT JOIN so that the results include stories that didn't receive any comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all stories posted on January 1, 2012, with number of comments\njoin_query = \"\"\"\n             WITH c AS\n             (\n             SELECT parent, COUNT(*) as num_comments\n             FROM `bigquery-public-data.hacker_news.comments` \n             GROUP BY parent\n             )\n             SELECT s.id as story_id, s.by, s.title, c.num_comments\n             FROM `bigquery-public-data.hacker_news.stories` AS s\n             LEFT JOIN c\n             ON s.id = c.parent\n             WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'\n             ORDER BY c.num_comments DESC\n             \"\"\"\n# So what are we doing with this query?\n    # We'll have two columns in our CTE, parents and num_comments and it's grouped by parents \n    # We then use c in our join query as the alias for the CTE created \n    # Remember that SQL queries aren't evaluate from top to bottom so can call out s. or c. before they are alias created\n    # The table that comes right after the JOIN call is the one that you are joining to (c)\n    # We filter by extracting date from time stamp and then we order \n\n# Run the query, and return a pandas DataFrame\njoin_result = client.query(join_query).result().to_dataframe()\njoin_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# None of these stories received any comments\njoin_result.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014. We use UNION DISTINCT (instead of UNION ALL) to ensure that each user appears in the table at most once."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all users who posted stories or comments on January 1, 2014\nunion_query = \"\"\"\n              SELECT c.by\n              FROM `bigquery-public-data.hacker_news.comments` AS c\n              WHERE EXTRACT(DATE FROM c.time_ts) = '2014-01-01'\n              UNION DISTINCT\n              SELECT s.by\n              FROM `bigquery-public-data.hacker_news.stories` AS s\n              WHERE EXTRACT(DATE FROM s.time_ts) = '2014-01-01'\n              \"\"\"\n\n# DISTINCT says I don't want any repeats and only unique values \n    # If looking at census data from different countries and you want to find out most common first name in world, \n    # you wouldn't want distinct\n    # We only care about the username here though, so DISTINCT makes sense. \n    # So we can filter and append at same time. \n\n# Run the query, and return a pandas DataFrame\nunion_result = client.query(union_query).result().to_dataframe()\nunion_result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get the number of users who posted on January 1, 2014, we need only take the length of the DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of users who posted stories or comments on January 1, 2014\nlen(union_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Day 10: Nested and Repeated Data"},{"metadata":{},"cell_type":"markdown","source":"How do we interact with hierarchical data? To query a column with nested data, we need to identify each field in the context of the column that contains it. So we just call it out with a dot. "},{"metadata":{},"cell_type":"markdown","source":"How do we interact with repeated data?  In this case, to collapse this information into a single table, we need to leverage a different datatype. We say that the \"Toys\" column contains repeated data, because it permits more than one value for each row. Each entry in a repeated field is an ARRAY, or an ordered list of (zero or more) values with the same datatype. When querying repeated data, we need to put the name of the column containing the repeated data inside an UNNEST() function. Keep in mind that we say the column name and then after from specify we want it unnested as that name we already called out. This essentially flattens the repeated data (which is then appended to the right side of the table) so that we have one element on each row."},{"metadata":{},"cell_type":"markdown","source":"What about nested and repeated data at the same time? We essentially use our unnest as an alias and then use the alias. notation. Since the \"Toys\" column is repeated, we flatten it with the UNNEST() function. And, since we give the flattened column an alias of t, we can refer to the \"Name\" and \"Type\" fields in the \"Toys\" column as t.Name and t.Type, respectively."},{"metadata":{},"cell_type":"markdown","source":"We'll work with the Google Analytics Sample dataset. It contains information tracking the behavior of visitors to the Google Merchandise store, an e-commerce website that sells Google branded items."},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"google_analytics_sample\" dataset\ndataset_ref = client.dataset(\"google_analytics_sample\", project=\"bigquery-public-data\")\n\n# Construct a reference to the \"ga_sessions_20170801\" table\ntable_ref = dataset_ref.table(\"ga_sessions_20170801\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table has many nested fields, which you can verify by looking at either the data dictionary (hint: search for appearances of 'RECORD' on the page) or the table preview above.\n\nIn our first query against this table, we'll work with the \"totals\" and \"device\" columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SCHEMA field for the 'totals' column:\\n\")\nprint(table.schema[5])\n# We are looking at totals\n\nprint(\"\\nSCHEMA field for the 'device' column:\\n\")\nprint(table.schema[7])\n# We are looking at device\n\n# Repeated data has square brackets around it. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We refer to the \"browser\" field (which is nested in the \"device\" column) and the \"transactions\" field (which is nested inside the \"totals\" column) as device.browser and totals.transactions in the query below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to count the number of transactions per browser\nquery = \"\"\"\n        SELECT device.browser AS device_browser,\n            SUM(totals.transactions) as total_transactions\n        FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`\n        GROUP BY device_browser\n        ORDER BY total_transactions DESC\n        \"\"\"\n# We're looking in the device column and then getting information from the browser field and then looking in the totals \n# columns and getting the transactions\n\n# Run the query, and return a pandas DataFrame\nresult = client.query(query).result().to_dataframe()\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll work with the \"hits\" column as an example of data that is both nested and repeated. Since:\n\n\"hits\" is a STRUCT (contains nested data) and is repeated,\n\"hitNumber\", \"page\", and \"type\" are all nested inside the \"hits\" column, and\n\"pagePath\" is nested inside the \"page\" field,\nwe can query these fields with the following syntax:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to determine most popular landing point on the website\nquery = \"\"\"\n        SELECT hits.page.pagePath as path,\n            COUNT(hits.page.pagePath) as counts\n        FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`, \n            UNNEST(hits) as hits\n        WHERE hits.type=\"PAGE\" and hits.hitNumber=1\n        GROUP BY path\n        ORDER BY counts DESC\n        \"\"\"\n\n# Hits is a record so we know it's a nested data structure. There are square brackets, so we know it's repeated too. \n# The data type of page is a record too. We have a record inside another record. page is nullable, so it's not repeated\n# Inside page, we have pagePath\n\n# We're unnesting hits, then filtering results with where, then group by path, and order by counts \n\n# Run the query, and return a pandas DataFrame\nresult = client.query(query).result().to_dataframe()\nresult.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}